{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1-W_vtzGtdcNXSd37j33mxNlYyDlKCCnD",
      "authorship_tag": "ABX9TyO0uXlKwQFjyJxE0q1G1Qnf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wayneotemah/AI-and-ML/blob/main/stable_diffutson.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow_addons\n",
        "!pip install ftfy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6BTNTFYEJ2O",
        "outputId": "0354bdb4-a139-4376-de6c-61d5ba125274"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow_addons\n",
            "  Downloading tensorflow_addons-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (611 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.8/611.8 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow_addons) (23.2)\n",
            "Collecting typeguard<3.0.0,>=2.7 (from tensorflow_addons)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard, tensorflow_addons\n",
            "Successfully installed tensorflow_addons-0.23.0 typeguard-2.13.3\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.3-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.4/53.4 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy) (0.2.12)\n",
            "Installing collected packages: ftfy\n",
            "Successfully installed ftfy-6.1.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "s5defETVDo1p"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import tensorflow_addons as tfa\n",
        "import numpy as np\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "from PIL.PngImagePlugin import PngInfo"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CONSTANTS"
      ],
      "metadata": {
        "id": "sWxts0TmKr0S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PYTORCH_CKPT_MAPPING = {'text_encoder': [('cond_stage_model.transformer.text_model.embeddings.token_embedding.weight',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.embeddings.position_embedding.weight',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.0.layer_norm1.weight',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.0.layer_norm1.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.q_proj.weight',\n",
        "   (1, 0)),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.q_proj.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.k_proj.weight',\n",
        "   (1, 0)),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.k_proj.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.v_proj.weight',\n",
        "   (1, 0)),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.v_proj.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.out_proj.weight',\n",
        "   (1, 0)),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.out_proj.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.0.layer_norm2.weight',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.0.layer_norm2.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.0.mlp.fc1.weight',\n",
        "   (1, 0)),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.0.mlp.fc1.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.0.mlp.fc2.weight',\n",
        "   (1, 0)),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.0.mlp.fc2.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.1.layer_norm1.weight',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.1.layer_norm1.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.q_proj.weight',\n",
        "   (1, 0)),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.q_proj.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.k_proj.weight',\n",
        "   (1, 0)),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.k_proj.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.v_proj.weight',\n",
        "   (1, 0)),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.v_proj.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.out_proj.weight',\n",
        "   (1, 0)),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.out_proj.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.1.layer_norm2.weight',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.1.layer_norm2.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.1.mlp.fc1.weight',\n",
        "   (1, 0)),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.1.mlp.fc1.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.1.mlp.fc2.weight',\n",
        "   (1, 0)),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.1.mlp.fc2.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.2.layer_norm1.weight',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.2.layer_norm1.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.q_proj.weight',\n",
        "   (1, 0)),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.q_proj.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.k_proj.weight',\n",
        "   (1, 0)),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.k_proj.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.v_proj.weight',\n",
        "   (1, 0)),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.v_proj.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.out_proj.weight',\n",
        "   (1, 0)),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.out_proj.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.2.layer_norm2.weight',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.2.layer_norm2.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.2.mlp.fc1.weight',\n",
        "   (1, 0)),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.2.mlp.fc1.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.2.mlp.fc2.weight',\n",
        "   (1, 0)),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.2.mlp.fc2.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.3.layer_norm1.weight',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.3.layer_norm1.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.q_proj.weight',\n",
        "   (1, 0)),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.q_proj.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.k_proj.weight',\n",
        "   (1, 0)),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.k_proj.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.v_proj.weight',\n",
        "   (1, 0)),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.v_proj.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.out_proj.weight',\n",
        "   (1, 0)),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.out_proj.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.3.layer_norm2.weight',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.3.layer_norm2.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.3.mlp.fc1.weight',\n",
        "   (1, 0)),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.3.mlp.fc1.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.3.mlp.fc2.weight',\n",
        "   (1, 0)),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.3.mlp.fc2.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.4.layer_norm1.weight',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.4.layer_norm1.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.q_proj.weight',\n",
        "   (1, 0)),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.q_proj.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.k_proj.weight',\n",
        "   (1, 0)),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.k_proj.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.v_proj.weight',\n",
        "   (1, 0)),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.v_proj.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.out_proj.weight',\n",
        "   (1, 0)),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.out_proj.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.4.layer_norm2.weight',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.4.layer_norm2.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.4.mlp.fc1.weight',\n",
        "   (1, 0)),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.4.mlp.fc1.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.4.mlp.fc2.weight',\n",
        "   (1, 0)),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.4.mlp.fc2.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.5.layer_norm1.weight',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.5.layer_norm1.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.q_proj.weight',\n",
        "   (1, 0)),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.q_proj.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.k_proj.weight',\n",
        "   (1, 0)),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.k_proj.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.v_proj.weight',\n",
        "   (1, 0)),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.v_proj.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.out_proj.weight',\n",
        "   (1, 0)),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.out_proj.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.5.layer_norm2.weight',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.5.layer_norm2.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.5.mlp.fc1.weight',\n",
        "   (1, 0)),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.5.mlp.fc1.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.5.mlp.fc2.weight',\n",
        "   (1, 0)),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.5.mlp.fc2.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.6.layer_norm1.weight',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.6.layer_norm1.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.q_proj.weight',\n",
        "   (1, 0)),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.q_proj.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.k_proj.weight',\n",
        "   (1, 0)),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.k_proj.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.v_proj.weight',\n",
        "   (1, 0)),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.v_proj.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.out_proj.weight',\n",
        "   (1, 0)),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.out_proj.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.6.layer_norm2.weight',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.6.layer_norm2.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.6.mlp.fc1.weight',\n",
        "   (1, 0)),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.6.mlp.fc1.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.6.mlp.fc2.weight',\n",
        "   (1, 0)),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.6.mlp.fc2.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.7.layer_norm1.weight',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.7.layer_norm1.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.q_proj.weight',\n",
        "   (1, 0)),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.q_proj.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.k_proj.weight',\n",
        "   (1, 0)),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.k_proj.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.v_proj.weight',\n",
        "   (1, 0)),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.v_proj.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.out_proj.weight',\n",
        "   (1, 0)),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.out_proj.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.7.layer_norm2.weight',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.7.layer_norm2.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.7.mlp.fc1.weight',\n",
        "   (1, 0)),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.7.mlp.fc1.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.7.mlp.fc2.weight',\n",
        "   (1, 0)),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.7.mlp.fc2.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.8.layer_norm1.weight',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.8.layer_norm1.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.q_proj.weight',\n",
        "   (1, 0)),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.q_proj.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.k_proj.weight',\n",
        "   (1, 0)),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.k_proj.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.v_proj.weight',\n",
        "   (1, 0)),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.v_proj.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.out_proj.weight',\n",
        "   (1, 0)),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.out_proj.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.8.layer_norm2.weight',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.8.layer_norm2.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.8.mlp.fc1.weight',\n",
        "   (1, 0)),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.8.mlp.fc1.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.8.mlp.fc2.weight',\n",
        "   (1, 0)),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.8.mlp.fc2.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.9.layer_norm1.weight',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.9.layer_norm1.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.q_proj.weight',\n",
        "   (1, 0)),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.q_proj.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.k_proj.weight',\n",
        "   (1, 0)),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.k_proj.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.v_proj.weight',\n",
        "   (1, 0)),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.v_proj.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.out_proj.weight',\n",
        "   (1, 0)),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.out_proj.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.9.layer_norm2.weight',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.9.layer_norm2.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.9.mlp.fc1.weight',\n",
        "   (1, 0)),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.9.mlp.fc1.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.9.mlp.fc2.weight',\n",
        "   (1, 0)),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.9.mlp.fc2.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.10.layer_norm1.weight',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.10.layer_norm1.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.q_proj.weight',\n",
        "   (1, 0)),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.q_proj.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.k_proj.weight',\n",
        "   (1, 0)),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.k_proj.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.v_proj.weight',\n",
        "   (1, 0)),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.v_proj.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.out_proj.weight',\n",
        "   (1, 0)),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.out_proj.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.10.layer_norm2.weight',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.10.layer_norm2.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.10.mlp.fc1.weight',\n",
        "   (1, 0)),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.10.mlp.fc1.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.10.mlp.fc2.weight',\n",
        "   (1, 0)),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.10.mlp.fc2.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.11.layer_norm1.weight',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.11.layer_norm1.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.q_proj.weight',\n",
        "   (1, 0)),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.q_proj.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.k_proj.weight',\n",
        "   (1, 0)),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.k_proj.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.v_proj.weight',\n",
        "   (1, 0)),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.v_proj.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.out_proj.weight',\n",
        "   (1, 0)),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.out_proj.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.11.layer_norm2.weight',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.11.layer_norm2.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.11.mlp.fc1.weight',\n",
        "   (1, 0)),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.11.mlp.fc1.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.11.mlp.fc2.weight',\n",
        "   (1, 0)),\n",
        "  ('cond_stage_model.transformer.text_model.encoder.layers.11.mlp.fc2.bias',\n",
        "   None),\n",
        "  ('cond_stage_model.transformer.text_model.final_layer_norm.weight', None),\n",
        "  ('cond_stage_model.transformer.text_model.final_layer_norm.bias', None)],\n",
        " 'diffusion_model': [('model.diffusion_model.time_embed.0.weight', (1, 0)),\n",
        "  ('model.diffusion_model.time_embed.0.bias', None),\n",
        "  ('model.diffusion_model.time_embed.2.weight', (1, 0)),\n",
        "  ('model.diffusion_model.time_embed.2.bias', None),\n",
        "  ('model.diffusion_model.input_blocks.0.0.weight', (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.0.0.bias', None),\n",
        "  ('model.diffusion_model.input_blocks.1.0.in_layers.0.weight', None),\n",
        "  ('model.diffusion_model.input_blocks.1.0.in_layers.0.bias', None),\n",
        "  ('model.diffusion_model.input_blocks.1.0.in_layers.2.weight', (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.1.0.in_layers.2.bias', None),\n",
        "  ('model.diffusion_model.input_blocks.1.0.emb_layers.1.weight', (1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.1.0.emb_layers.1.bias', None),\n",
        "  ('model.diffusion_model.input_blocks.1.0.out_layers.0.weight', None),\n",
        "  ('model.diffusion_model.input_blocks.1.0.out_layers.0.bias', None),\n",
        "  ('model.diffusion_model.input_blocks.1.0.out_layers.3.weight', (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.1.0.out_layers.3.bias', None),\n",
        "  ('model.diffusion_model.input_blocks.1.1.norm.weight', None),\n",
        "  ('model.diffusion_model.input_blocks.1.1.norm.bias', None),\n",
        "  ('model.diffusion_model.input_blocks.1.1.proj_in.weight', (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.1.1.proj_in.bias', None),\n",
        "  ('model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm1.weight',\n",
        "   None),\n",
        "  ('model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm1.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_q.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_k.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_v.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm2.weight',\n",
        "   None),\n",
        "  ('model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm2.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_q.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_k.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_v.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm3.weight',\n",
        "   None),\n",
        "  ('model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm3.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.input_blocks.1.1.proj_out.weight', (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.1.1.proj_out.bias', None),\n",
        "  ('model.diffusion_model.input_blocks.2.0.in_layers.0.weight', None),\n",
        "  ('model.diffusion_model.input_blocks.2.0.in_layers.0.bias', None),\n",
        "  ('model.diffusion_model.input_blocks.2.0.in_layers.2.weight', (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.2.0.in_layers.2.bias', None),\n",
        "  ('model.diffusion_model.input_blocks.2.0.emb_layers.1.weight', (1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.2.0.emb_layers.1.bias', None),\n",
        "  ('model.diffusion_model.input_blocks.2.0.out_layers.0.weight', None),\n",
        "  ('model.diffusion_model.input_blocks.2.0.out_layers.0.bias', None),\n",
        "  ('model.diffusion_model.input_blocks.2.0.out_layers.3.weight', (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.2.0.out_layers.3.bias', None),\n",
        "  ('model.diffusion_model.input_blocks.2.1.norm.weight', None),\n",
        "  ('model.diffusion_model.input_blocks.2.1.norm.bias', None),\n",
        "  ('model.diffusion_model.input_blocks.2.1.proj_in.weight', (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.2.1.proj_in.bias', None),\n",
        "  ('model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm1.weight',\n",
        "   None),\n",
        "  ('model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm1.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_q.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_k.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_v.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm2.weight',\n",
        "   None),\n",
        "  ('model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm2.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_q.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_k.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_v.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm3.weight',\n",
        "   None),\n",
        "  ('model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm3.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.input_blocks.2.1.proj_out.weight', (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.2.1.proj_out.bias', None),\n",
        "  ('model.diffusion_model.input_blocks.3.0.op.weight', (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.3.0.op.bias', None),\n",
        "  ('model.diffusion_model.input_blocks.4.0.in_layers.0.weight', None),\n",
        "  ('model.diffusion_model.input_blocks.4.0.in_layers.0.bias', None),\n",
        "  ('model.diffusion_model.input_blocks.4.0.in_layers.2.weight', (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.4.0.in_layers.2.bias', None),\n",
        "  ('model.diffusion_model.input_blocks.4.0.emb_layers.1.weight', (1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.4.0.emb_layers.1.bias', None),\n",
        "  ('model.diffusion_model.input_blocks.4.0.out_layers.0.weight', None),\n",
        "  ('model.diffusion_model.input_blocks.4.0.out_layers.0.bias', None),\n",
        "  ('model.diffusion_model.input_blocks.4.0.out_layers.3.weight', (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.4.0.out_layers.3.bias', None),\n",
        "  ('model.diffusion_model.input_blocks.4.0.skip_connection.weight',\n",
        "   (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.4.0.skip_connection.bias', None),\n",
        "  ('model.diffusion_model.input_blocks.4.1.norm.weight', None),\n",
        "  ('model.diffusion_model.input_blocks.4.1.norm.bias', None),\n",
        "  ('model.diffusion_model.input_blocks.4.1.proj_in.weight', (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.4.1.proj_in.bias', None),\n",
        "  ('model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm1.weight',\n",
        "   None),\n",
        "  ('model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm1.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_q.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_k.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_v.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm2.weight',\n",
        "   None),\n",
        "  ('model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm2.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_q.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_k.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_v.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm3.weight',\n",
        "   None),\n",
        "  ('model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm3.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.input_blocks.4.1.proj_out.weight', (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.4.1.proj_out.bias', None),\n",
        "  ('model.diffusion_model.input_blocks.5.0.in_layers.0.weight', None),\n",
        "  ('model.diffusion_model.input_blocks.5.0.in_layers.0.bias', None),\n",
        "  ('model.diffusion_model.input_blocks.5.0.in_layers.2.weight', (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.5.0.in_layers.2.bias', None),\n",
        "  ('model.diffusion_model.input_blocks.5.0.emb_layers.1.weight', (1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.5.0.emb_layers.1.bias', None),\n",
        "  ('model.diffusion_model.input_blocks.5.0.out_layers.0.weight', None),\n",
        "  ('model.diffusion_model.input_blocks.5.0.out_layers.0.bias', None),\n",
        "  ('model.diffusion_model.input_blocks.5.0.out_layers.3.weight', (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.5.0.out_layers.3.bias', None),\n",
        "  ('model.diffusion_model.input_blocks.5.1.norm.weight', None),\n",
        "  ('model.diffusion_model.input_blocks.5.1.norm.bias', None),\n",
        "  ('model.diffusion_model.input_blocks.5.1.proj_in.weight', (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.5.1.proj_in.bias', None),\n",
        "  ('model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm1.weight',\n",
        "   None),\n",
        "  ('model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm1.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_q.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_k.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_v.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm2.weight',\n",
        "   None),\n",
        "  ('model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm2.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_q.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_k.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_v.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm3.weight',\n",
        "   None),\n",
        "  ('model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm3.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.input_blocks.5.1.proj_out.weight', (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.5.1.proj_out.bias', None),\n",
        "  ('model.diffusion_model.input_blocks.6.0.op.weight', (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.6.0.op.bias', None),\n",
        "  ('model.diffusion_model.input_blocks.7.0.in_layers.0.weight', None),\n",
        "  ('model.diffusion_model.input_blocks.7.0.in_layers.0.bias', None),\n",
        "  ('model.diffusion_model.input_blocks.7.0.in_layers.2.weight', (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.7.0.in_layers.2.bias', None),\n",
        "  ('model.diffusion_model.input_blocks.7.0.emb_layers.1.weight', (1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.7.0.emb_layers.1.bias', None),\n",
        "  ('model.diffusion_model.input_blocks.7.0.out_layers.0.weight', None),\n",
        "  ('model.diffusion_model.input_blocks.7.0.out_layers.0.bias', None),\n",
        "  ('model.diffusion_model.input_blocks.7.0.out_layers.3.weight', (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.7.0.out_layers.3.bias', None),\n",
        "  ('model.diffusion_model.input_blocks.7.0.skip_connection.weight',\n",
        "   (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.7.0.skip_connection.bias', None),\n",
        "  ('model.diffusion_model.input_blocks.7.1.norm.weight', None),\n",
        "  ('model.diffusion_model.input_blocks.7.1.norm.bias', None),\n",
        "  ('model.diffusion_model.input_blocks.7.1.proj_in.weight', (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.7.1.proj_in.bias', None),\n",
        "  ('model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm1.weight',\n",
        "   None),\n",
        "  ('model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm1.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_q.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_k.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_v.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm2.weight',\n",
        "   None),\n",
        "  ('model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm2.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_q.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_k.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_v.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm3.weight',\n",
        "   None),\n",
        "  ('model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm3.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.input_blocks.7.1.proj_out.weight', (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.7.1.proj_out.bias', None),\n",
        "  ('model.diffusion_model.input_blocks.8.0.in_layers.0.weight', None),\n",
        "  ('model.diffusion_model.input_blocks.8.0.in_layers.0.bias', None),\n",
        "  ('model.diffusion_model.input_blocks.8.0.in_layers.2.weight', (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.8.0.in_layers.2.bias', None),\n",
        "  ('model.diffusion_model.input_blocks.8.0.emb_layers.1.weight', (1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.8.0.emb_layers.1.bias', None),\n",
        "  ('model.diffusion_model.input_blocks.8.0.out_layers.0.weight', None),\n",
        "  ('model.diffusion_model.input_blocks.8.0.out_layers.0.bias', None),\n",
        "  ('model.diffusion_model.input_blocks.8.0.out_layers.3.weight', (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.8.0.out_layers.3.bias', None),\n",
        "  ('model.diffusion_model.input_blocks.8.1.norm.weight', None),\n",
        "  ('model.diffusion_model.input_blocks.8.1.norm.bias', None),\n",
        "  ('model.diffusion_model.input_blocks.8.1.proj_in.weight', (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.8.1.proj_in.bias', None),\n",
        "  ('model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm1.weight',\n",
        "   None),\n",
        "  ('model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm1.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_q.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_k.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_v.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm2.weight',\n",
        "   None),\n",
        "  ('model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm2.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_q.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_k.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_v.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm3.weight',\n",
        "   None),\n",
        "  ('model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm3.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.input_blocks.8.1.proj_out.weight', (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.8.1.proj_out.bias', None),\n",
        "  ('model.diffusion_model.input_blocks.9.0.op.weight', (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.9.0.op.bias', None),\n",
        "  ('model.diffusion_model.input_blocks.10.0.in_layers.0.weight', None),\n",
        "  ('model.diffusion_model.input_blocks.10.0.in_layers.0.bias', None),\n",
        "  ('model.diffusion_model.input_blocks.10.0.in_layers.2.weight', (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.10.0.in_layers.2.bias', None),\n",
        "  ('model.diffusion_model.input_blocks.10.0.emb_layers.1.weight', (1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.10.0.emb_layers.1.bias', None),\n",
        "  ('model.diffusion_model.input_blocks.10.0.out_layers.0.weight', None),\n",
        "  ('model.diffusion_model.input_blocks.10.0.out_layers.0.bias', None),\n",
        "  ('model.diffusion_model.input_blocks.10.0.out_layers.3.weight',\n",
        "   (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.10.0.out_layers.3.bias', None),\n",
        "  ('model.diffusion_model.input_blocks.11.0.in_layers.0.weight', None),\n",
        "  ('model.diffusion_model.input_blocks.11.0.in_layers.0.bias', None),\n",
        "  ('model.diffusion_model.input_blocks.11.0.in_layers.2.weight', (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.11.0.in_layers.2.bias', None),\n",
        "  ('model.diffusion_model.input_blocks.11.0.emb_layers.1.weight', (1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.11.0.emb_layers.1.bias', None),\n",
        "  ('model.diffusion_model.input_blocks.11.0.out_layers.0.weight', None),\n",
        "  ('model.diffusion_model.input_blocks.11.0.out_layers.0.bias', None),\n",
        "  ('model.diffusion_model.input_blocks.11.0.out_layers.3.weight',\n",
        "   (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.input_blocks.11.0.out_layers.3.bias', None),\n",
        "  ('model.diffusion_model.middle_block.0.in_layers.0.weight', None),\n",
        "  ('model.diffusion_model.middle_block.0.in_layers.0.bias', None),\n",
        "  ('model.diffusion_model.middle_block.0.in_layers.2.weight', (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.middle_block.0.in_layers.2.bias', None),\n",
        "  ('model.diffusion_model.middle_block.0.emb_layers.1.weight', (1, 0)),\n",
        "  ('model.diffusion_model.middle_block.0.emb_layers.1.bias', None),\n",
        "  ('model.diffusion_model.middle_block.0.out_layers.0.weight', None),\n",
        "  ('model.diffusion_model.middle_block.0.out_layers.0.bias', None),\n",
        "  ('model.diffusion_model.middle_block.0.out_layers.3.weight', (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.middle_block.0.out_layers.3.bias', None),\n",
        "  ('model.diffusion_model.middle_block.1.norm.weight', None),\n",
        "  ('model.diffusion_model.middle_block.1.norm.bias', None),\n",
        "  ('model.diffusion_model.middle_block.1.proj_in.weight', (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.middle_block.1.proj_in.bias', None),\n",
        "  ('model.diffusion_model.middle_block.1.transformer_blocks.0.norm1.weight',\n",
        "   None),\n",
        "  ('model.diffusion_model.middle_block.1.transformer_blocks.0.norm1.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_q.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_k.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_v.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.middle_block.1.transformer_blocks.0.norm2.weight',\n",
        "   None),\n",
        "  ('model.diffusion_model.middle_block.1.transformer_blocks.0.norm2.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_q.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_k.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_v.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.middle_block.1.transformer_blocks.0.norm3.weight',\n",
        "   None),\n",
        "  ('model.diffusion_model.middle_block.1.transformer_blocks.0.norm3.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.2.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.2.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.middle_block.1.proj_out.weight', (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.middle_block.1.proj_out.bias', None),\n",
        "  ('model.diffusion_model.middle_block.2.in_layers.0.weight', None),\n",
        "  ('model.diffusion_model.middle_block.2.in_layers.0.bias', None),\n",
        "  ('model.diffusion_model.middle_block.2.in_layers.2.weight', (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.middle_block.2.in_layers.2.bias', None),\n",
        "  ('model.diffusion_model.middle_block.2.emb_layers.1.weight', (1, 0)),\n",
        "  ('model.diffusion_model.middle_block.2.emb_layers.1.bias', None),\n",
        "  ('model.diffusion_model.middle_block.2.out_layers.0.weight', None),\n",
        "  ('model.diffusion_model.middle_block.2.out_layers.0.bias', None),\n",
        "  ('model.diffusion_model.middle_block.2.out_layers.3.weight', (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.middle_block.2.out_layers.3.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.0.0.in_layers.0.weight', None),\n",
        "  ('model.diffusion_model.output_blocks.0.0.in_layers.0.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.0.0.in_layers.2.weight', (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.0.0.in_layers.2.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.0.0.emb_layers.1.weight', (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.0.0.emb_layers.1.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.0.0.out_layers.0.weight', None),\n",
        "  ('model.diffusion_model.output_blocks.0.0.out_layers.0.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.0.0.out_layers.3.weight',\n",
        "   (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.0.0.out_layers.3.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.0.0.skip_connection.weight',\n",
        "   (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.0.0.skip_connection.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.1.0.in_layers.0.weight', None),\n",
        "  ('model.diffusion_model.output_blocks.1.0.in_layers.0.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.1.0.in_layers.2.weight', (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.1.0.in_layers.2.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.1.0.emb_layers.1.weight', (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.1.0.emb_layers.1.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.1.0.out_layers.0.weight', None),\n",
        "  ('model.diffusion_model.output_blocks.1.0.out_layers.0.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.1.0.out_layers.3.weight',\n",
        "   (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.1.0.out_layers.3.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.1.0.skip_connection.weight',\n",
        "   (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.1.0.skip_connection.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.2.0.in_layers.0.weight', None),\n",
        "  ('model.diffusion_model.output_blocks.2.0.in_layers.0.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.2.0.in_layers.2.weight', (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.2.0.in_layers.2.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.2.0.emb_layers.1.weight', (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.2.0.emb_layers.1.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.2.0.out_layers.0.weight', None),\n",
        "  ('model.diffusion_model.output_blocks.2.0.out_layers.0.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.2.0.out_layers.3.weight',\n",
        "   (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.2.0.out_layers.3.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.2.0.skip_connection.weight',\n",
        "   (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.2.0.skip_connection.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.2.1.conv.weight', (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.2.1.conv.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.3.0.in_layers.0.weight', None),\n",
        "  ('model.diffusion_model.output_blocks.3.0.in_layers.0.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.3.0.in_layers.2.weight', (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.3.0.in_layers.2.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.3.0.emb_layers.1.weight', (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.3.0.emb_layers.1.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.3.0.out_layers.0.weight', None),\n",
        "  ('model.diffusion_model.output_blocks.3.0.out_layers.0.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.3.0.out_layers.3.weight',\n",
        "   (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.3.0.out_layers.3.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.3.0.skip_connection.weight',\n",
        "   (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.3.0.skip_connection.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.3.1.norm.weight', None),\n",
        "  ('model.diffusion_model.output_blocks.3.1.norm.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.3.1.proj_in.weight', (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.3.1.proj_in.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm1.weight',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm1.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_q.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_k.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_v.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm2.weight',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm2.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_q.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_k.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_v.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm3.weight',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm3.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.2.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.2.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.3.1.proj_out.weight', (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.3.1.proj_out.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.4.0.in_layers.0.weight', None),\n",
        "  ('model.diffusion_model.output_blocks.4.0.in_layers.0.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.4.0.in_layers.2.weight', (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.4.0.in_layers.2.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.4.0.emb_layers.1.weight', (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.4.0.emb_layers.1.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.4.0.out_layers.0.weight', None),\n",
        "  ('model.diffusion_model.output_blocks.4.0.out_layers.0.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.4.0.out_layers.3.weight',\n",
        "   (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.4.0.out_layers.3.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.4.0.skip_connection.weight',\n",
        "   (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.4.0.skip_connection.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.4.1.norm.weight', None),\n",
        "  ('model.diffusion_model.output_blocks.4.1.norm.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.4.1.proj_in.weight', (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.4.1.proj_in.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm1.weight',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm1.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_q.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_k.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_v.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm2.weight',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm2.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_q.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_k.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_v.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm3.weight',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm3.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.2.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.2.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.4.1.proj_out.weight', (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.4.1.proj_out.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.5.0.in_layers.0.weight', None),\n",
        "  ('model.diffusion_model.output_blocks.5.0.in_layers.0.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.5.0.in_layers.2.weight', (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.5.0.in_layers.2.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.5.0.emb_layers.1.weight', (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.5.0.emb_layers.1.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.5.0.out_layers.0.weight', None),\n",
        "  ('model.diffusion_model.output_blocks.5.0.out_layers.0.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.5.0.out_layers.3.weight',\n",
        "   (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.5.0.out_layers.3.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.5.0.skip_connection.weight',\n",
        "   (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.5.0.skip_connection.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.5.1.norm.weight', None),\n",
        "  ('model.diffusion_model.output_blocks.5.1.norm.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.5.1.proj_in.weight', (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.5.1.proj_in.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm1.weight',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm1.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_q.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_k.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_v.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm2.weight',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm2.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_q.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_k.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_v.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm3.weight',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm3.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.2.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.2.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.5.1.proj_out.weight', (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.5.1.proj_out.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.5.2.conv.weight', (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.5.2.conv.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.6.0.in_layers.0.weight', None),\n",
        "  ('model.diffusion_model.output_blocks.6.0.in_layers.0.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.6.0.in_layers.2.weight', (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.6.0.in_layers.2.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.6.0.emb_layers.1.weight', (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.6.0.emb_layers.1.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.6.0.out_layers.0.weight', None),\n",
        "  ('model.diffusion_model.output_blocks.6.0.out_layers.0.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.6.0.out_layers.3.weight',\n",
        "   (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.6.0.out_layers.3.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.6.0.skip_connection.weight',\n",
        "   (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.6.0.skip_connection.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.6.1.norm.weight', None),\n",
        "  ('model.diffusion_model.output_blocks.6.1.norm.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.6.1.proj_in.weight', (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.6.1.proj_in.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm1.weight',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm1.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_q.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_k.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_v.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm2.weight',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm2.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_q.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_k.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_v.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm3.weight',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm3.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.2.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.2.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.6.1.proj_out.weight', (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.6.1.proj_out.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.7.0.in_layers.0.weight', None),\n",
        "  ('model.diffusion_model.output_blocks.7.0.in_layers.0.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.7.0.in_layers.2.weight', (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.7.0.in_layers.2.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.7.0.emb_layers.1.weight', (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.7.0.emb_layers.1.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.7.0.out_layers.0.weight', None),\n",
        "  ('model.diffusion_model.output_blocks.7.0.out_layers.0.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.7.0.out_layers.3.weight',\n",
        "   (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.7.0.out_layers.3.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.7.0.skip_connection.weight',\n",
        "   (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.7.0.skip_connection.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.7.1.norm.weight', None),\n",
        "  ('model.diffusion_model.output_blocks.7.1.norm.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.7.1.proj_in.weight', (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.7.1.proj_in.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm1.weight',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm1.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_q.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_k.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_v.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm2.weight',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm2.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_q.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_k.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_v.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm3.weight',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm3.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.2.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.2.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.7.1.proj_out.weight', (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.7.1.proj_out.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.8.0.in_layers.0.weight', None),\n",
        "  ('model.diffusion_model.output_blocks.8.0.in_layers.0.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.8.0.in_layers.2.weight', (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.8.0.in_layers.2.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.8.0.emb_layers.1.weight', (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.8.0.emb_layers.1.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.8.0.out_layers.0.weight', None),\n",
        "  ('model.diffusion_model.output_blocks.8.0.out_layers.0.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.8.0.out_layers.3.weight',\n",
        "   (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.8.0.out_layers.3.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.8.0.skip_connection.weight',\n",
        "   (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.8.0.skip_connection.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.8.1.norm.weight', None),\n",
        "  ('model.diffusion_model.output_blocks.8.1.norm.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.8.1.proj_in.weight', (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.8.1.proj_in.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm1.weight',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm1.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_q.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_k.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_v.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm2.weight',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm2.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_q.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_k.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_v.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm3.weight',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm3.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.2.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.2.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.8.1.proj_out.weight', (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.8.1.proj_out.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.8.2.conv.weight', (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.8.2.conv.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.9.0.in_layers.0.weight', None),\n",
        "  ('model.diffusion_model.output_blocks.9.0.in_layers.0.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.9.0.in_layers.2.weight', (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.9.0.in_layers.2.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.9.0.emb_layers.1.weight', (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.9.0.emb_layers.1.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.9.0.out_layers.0.weight', None),\n",
        "  ('model.diffusion_model.output_blocks.9.0.out_layers.0.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.9.0.out_layers.3.weight',\n",
        "   (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.9.0.out_layers.3.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.9.0.skip_connection.weight',\n",
        "   (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.9.0.skip_connection.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.9.1.norm.weight', None),\n",
        "  ('model.diffusion_model.output_blocks.9.1.norm.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.9.1.proj_in.weight', (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.9.1.proj_in.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm1.weight',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm1.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_q.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_k.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_v.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm2.weight',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm2.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_q.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_k.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_v.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm3.weight',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm3.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.2.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.2.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.9.1.proj_out.weight', (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.9.1.proj_out.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.10.0.in_layers.0.weight', None),\n",
        "  ('model.diffusion_model.output_blocks.10.0.in_layers.0.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.10.0.in_layers.2.weight',\n",
        "   (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.10.0.in_layers.2.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.10.0.emb_layers.1.weight', (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.10.0.emb_layers.1.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.10.0.out_layers.0.weight', None),\n",
        "  ('model.diffusion_model.output_blocks.10.0.out_layers.0.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.10.0.out_layers.3.weight',\n",
        "   (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.10.0.out_layers.3.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.10.0.skip_connection.weight',\n",
        "   (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.10.0.skip_connection.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.10.1.norm.weight', None),\n",
        "  ('model.diffusion_model.output_blocks.10.1.norm.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.10.1.proj_in.weight', (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.10.1.proj_in.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm1.weight',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm1.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_q.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_k.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_v.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm2.weight',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm2.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_q.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_k.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_v.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm3.weight',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm3.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.2.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.2.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.10.1.proj_out.weight', (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.10.1.proj_out.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.11.0.in_layers.0.weight', None),\n",
        "  ('model.diffusion_model.output_blocks.11.0.in_layers.0.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.11.0.in_layers.2.weight',\n",
        "   (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.11.0.in_layers.2.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.11.0.emb_layers.1.weight', (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.11.0.emb_layers.1.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.11.0.out_layers.0.weight', None),\n",
        "  ('model.diffusion_model.output_blocks.11.0.out_layers.0.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.11.0.out_layers.3.weight',\n",
        "   (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.11.0.out_layers.3.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.11.0.skip_connection.weight',\n",
        "   (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.11.0.skip_connection.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.11.1.norm.weight', None),\n",
        "  ('model.diffusion_model.output_blocks.11.1.norm.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.11.1.proj_in.weight', (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.11.1.proj_in.bias', None),\n",
        "  ('model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm1.weight',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm1.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_q.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_k.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_v.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm2.weight',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm2.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_q.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_k.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_v.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm3.weight',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm3.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.0.proj.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.0.proj.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.2.weight',\n",
        "   (1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.2.bias',\n",
        "   None),\n",
        "  ('model.diffusion_model.output_blocks.11.1.proj_out.weight', (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.output_blocks.11.1.proj_out.bias', None),\n",
        "  ('model.diffusion_model.out.0.weight', None),\n",
        "  ('model.diffusion_model.out.0.bias', None),\n",
        "  ('model.diffusion_model.out.2.weight', (2, 3, 1, 0)),\n",
        "  ('model.diffusion_model.out.2.bias', None)],\n",
        " 'decoder': [('first_stage_model.post_quant_conv.weight', (2, 3, 1, 0)),\n",
        "  ('first_stage_model.post_quant_conv.bias', None),\n",
        "  ('first_stage_model.decoder.conv_in.weight', (2, 3, 1, 0)),\n",
        "  ('first_stage_model.decoder.conv_in.bias', None),\n",
        "  ('first_stage_model.decoder.mid.block_1.norm1.weight', None),\n",
        "  ('first_stage_model.decoder.mid.block_1.norm1.bias', None),\n",
        "  ('first_stage_model.decoder.mid.block_1.conv1.weight', (2, 3, 1, 0)),\n",
        "  ('first_stage_model.decoder.mid.block_1.conv1.bias', None),\n",
        "  ('first_stage_model.decoder.mid.block_1.norm2.weight', None),\n",
        "  ('first_stage_model.decoder.mid.block_1.norm2.bias', None),\n",
        "  ('first_stage_model.decoder.mid.block_1.conv2.weight', (2, 3, 1, 0)),\n",
        "  ('first_stage_model.decoder.mid.block_1.conv2.bias', None),\n",
        "  ('first_stage_model.decoder.mid.attn_1.norm.weight', None),\n",
        "  ('first_stage_model.decoder.mid.attn_1.norm.bias', None),\n",
        "  ('first_stage_model.decoder.mid.attn_1.q.weight', (2, 3, 1, 0)),\n",
        "  ('first_stage_model.decoder.mid.attn_1.q.bias', None),\n",
        "  ('first_stage_model.decoder.mid.attn_1.k.weight', (2, 3, 1, 0)),\n",
        "  ('first_stage_model.decoder.mid.attn_1.k.bias', None),\n",
        "  ('first_stage_model.decoder.mid.attn_1.v.weight', (2, 3, 1, 0)),\n",
        "  ('first_stage_model.decoder.mid.attn_1.v.bias', None),\n",
        "  ('first_stage_model.decoder.mid.attn_1.proj_out.weight', (2, 3, 1, 0)),\n",
        "  ('first_stage_model.decoder.mid.attn_1.proj_out.bias', None),\n",
        "  ('first_stage_model.decoder.mid.block_2.norm1.weight', None),\n",
        "  ('first_stage_model.decoder.mid.block_2.norm1.bias', None),\n",
        "  ('first_stage_model.decoder.mid.block_2.conv1.weight', (2, 3, 1, 0)),\n",
        "  ('first_stage_model.decoder.mid.block_2.conv1.bias', None),\n",
        "  ('first_stage_model.decoder.mid.block_2.norm2.weight', None),\n",
        "  ('first_stage_model.decoder.mid.block_2.norm2.bias', None),\n",
        "  ('first_stage_model.decoder.mid.block_2.conv2.weight', (2, 3, 1, 0)),\n",
        "  ('first_stage_model.decoder.mid.block_2.conv2.bias', None),\n",
        "  ('first_stage_model.decoder.up.3.block.0.norm1.weight', None),\n",
        "  ('first_stage_model.decoder.up.3.block.0.norm1.bias', None),\n",
        "  ('first_stage_model.decoder.up.3.block.0.conv1.weight', (2, 3, 1, 0)),\n",
        "  ('first_stage_model.decoder.up.3.block.0.conv1.bias', None),\n",
        "  ('first_stage_model.decoder.up.3.block.0.norm2.weight', None),\n",
        "  ('first_stage_model.decoder.up.3.block.0.norm2.bias', None),\n",
        "  ('first_stage_model.decoder.up.3.block.0.conv2.weight', (2, 3, 1, 0)),\n",
        "  ('first_stage_model.decoder.up.3.block.0.conv2.bias', None),\n",
        "  ('first_stage_model.decoder.up.3.block.1.norm1.weight', None),\n",
        "  ('first_stage_model.decoder.up.3.block.1.norm1.bias', None),\n",
        "  ('first_stage_model.decoder.up.3.block.1.conv1.weight', (2, 3, 1, 0)),\n",
        "  ('first_stage_model.decoder.up.3.block.1.conv1.bias', None),\n",
        "  ('first_stage_model.decoder.up.3.block.1.norm2.weight', None),\n",
        "  ('first_stage_model.decoder.up.3.block.1.norm2.bias', None),\n",
        "  ('first_stage_model.decoder.up.3.block.1.conv2.weight', (2, 3, 1, 0)),\n",
        "  ('first_stage_model.decoder.up.3.block.1.conv2.bias', None),\n",
        "  ('first_stage_model.decoder.up.3.block.2.norm1.weight', None),\n",
        "  ('first_stage_model.decoder.up.3.block.2.norm1.bias', None),\n",
        "  ('first_stage_model.decoder.up.3.block.2.conv1.weight', (2, 3, 1, 0)),\n",
        "  ('first_stage_model.decoder.up.3.block.2.conv1.bias', None),\n",
        "  ('first_stage_model.decoder.up.3.block.2.norm2.weight', None),\n",
        "  ('first_stage_model.decoder.up.3.block.2.norm2.bias', None),\n",
        "  ('first_stage_model.decoder.up.3.block.2.conv2.weight', (2, 3, 1, 0)),\n",
        "  ('first_stage_model.decoder.up.3.block.2.conv2.bias', None),\n",
        "  ('first_stage_model.decoder.up.3.upsample.conv.weight', (2, 3, 1, 0)),\n",
        "  ('first_stage_model.decoder.up.3.upsample.conv.bias', None),\n",
        "  ('first_stage_model.decoder.up.2.block.0.norm1.weight', None),\n",
        "  ('first_stage_model.decoder.up.2.block.0.norm1.bias', None),\n",
        "  ('first_stage_model.decoder.up.2.block.0.conv1.weight', (2, 3, 1, 0)),\n",
        "  ('first_stage_model.decoder.up.2.block.0.conv1.bias', None),\n",
        "  ('first_stage_model.decoder.up.2.block.0.norm2.weight', None),\n",
        "  ('first_stage_model.decoder.up.2.block.0.norm2.bias', None),\n",
        "  ('first_stage_model.decoder.up.2.block.0.conv2.weight', (2, 3, 1, 0)),\n",
        "  ('first_stage_model.decoder.up.2.block.0.conv2.bias', None),\n",
        "  ('first_stage_model.decoder.up.2.block.1.norm1.weight', None),\n",
        "  ('first_stage_model.decoder.up.2.block.1.norm1.bias', None),\n",
        "  ('first_stage_model.decoder.up.2.block.1.conv1.weight', (2, 3, 1, 0)),\n",
        "  ('first_stage_model.decoder.up.2.block.1.conv1.bias', None),\n",
        "  ('first_stage_model.decoder.up.2.block.1.norm2.weight', None),\n",
        "  ('first_stage_model.decoder.up.2.block.1.norm2.bias', None),\n",
        "  ('first_stage_model.decoder.up.2.block.1.conv2.weight', (2, 3, 1, 0)),\n",
        "  ('first_stage_model.decoder.up.2.block.1.conv2.bias', None),\n",
        "  ('first_stage_model.decoder.up.2.block.2.norm1.weight', None),\n",
        "  ('first_stage_model.decoder.up.2.block.2.norm1.bias', None),\n",
        "  ('first_stage_model.decoder.up.2.block.2.conv1.weight', (2, 3, 1, 0)),\n",
        "  ('first_stage_model.decoder.up.2.block.2.conv1.bias', None),\n",
        "  ('first_stage_model.decoder.up.2.block.2.norm2.weight', None),\n",
        "  ('first_stage_model.decoder.up.2.block.2.norm2.bias', None),\n",
        "  ('first_stage_model.decoder.up.2.block.2.conv2.weight', (2, 3, 1, 0)),\n",
        "  ('first_stage_model.decoder.up.2.block.2.conv2.bias', None),\n",
        "  ('first_stage_model.decoder.up.2.upsample.conv.weight', (2, 3, 1, 0)),\n",
        "  ('first_stage_model.decoder.up.2.upsample.conv.bias', None),\n",
        "  ('first_stage_model.decoder.up.1.block.0.norm1.weight', None),\n",
        "  ('first_stage_model.decoder.up.1.block.0.norm1.bias', None),\n",
        "  ('first_stage_model.decoder.up.1.block.0.conv1.weight', (2, 3, 1, 0)),\n",
        "  ('first_stage_model.decoder.up.1.block.0.conv1.bias', None),\n",
        "  ('first_stage_model.decoder.up.1.block.0.norm2.weight', None),\n",
        "  ('first_stage_model.decoder.up.1.block.0.norm2.bias', None),\n",
        "  ('first_stage_model.decoder.up.1.block.0.conv2.weight', (2, 3, 1, 0)),\n",
        "  ('first_stage_model.decoder.up.1.block.0.conv2.bias', None),\n",
        "  ('first_stage_model.decoder.up.1.block.0.nin_shortcut.weight', (2, 3, 1, 0)),\n",
        "  ('first_stage_model.decoder.up.1.block.0.nin_shortcut.bias', None),\n",
        "  ('first_stage_model.decoder.up.1.block.1.norm1.weight', None),\n",
        "  ('first_stage_model.decoder.up.1.block.1.norm1.bias', None),\n",
        "  ('first_stage_model.decoder.up.1.block.1.conv1.weight', (2, 3, 1, 0)),\n",
        "  ('first_stage_model.decoder.up.1.block.1.conv1.bias', None),\n",
        "  ('first_stage_model.decoder.up.1.block.1.norm2.weight', None),\n",
        "  ('first_stage_model.decoder.up.1.block.1.norm2.bias', None),\n",
        "  ('first_stage_model.decoder.up.1.block.1.conv2.weight', (2, 3, 1, 0)),\n",
        "  ('first_stage_model.decoder.up.1.block.1.conv2.bias', None),\n",
        "  ('first_stage_model.decoder.up.1.block.2.norm1.weight', None),\n",
        "  ('first_stage_model.decoder.up.1.block.2.norm1.bias', None),\n",
        "  ('first_stage_model.decoder.up.1.block.2.conv1.weight', (2, 3, 1, 0)),\n",
        "  ('first_stage_model.decoder.up.1.block.2.conv1.bias', None),\n",
        "  ('first_stage_model.decoder.up.1.block.2.norm2.weight', None),\n",
        "  ('first_stage_model.decoder.up.1.block.2.norm2.bias', None),\n",
        "  ('first_stage_model.decoder.up.1.block.2.conv2.weight', (2, 3, 1, 0)),\n",
        "  ('first_stage_model.decoder.up.1.block.2.conv2.bias', None),\n",
        "  ('first_stage_model.decoder.up.1.upsample.conv.weight', (2, 3, 1, 0)),\n",
        "  ('first_stage_model.decoder.up.1.upsample.conv.bias', None),\n",
        "  ('first_stage_model.decoder.up.0.block.0.norm1.weight', None),\n",
        "  ('first_stage_model.decoder.up.0.block.0.norm1.bias', None),\n",
        "  ('first_stage_model.decoder.up.0.block.0.conv1.weight', (2, 3, 1, 0)),\n",
        "  ('first_stage_model.decoder.up.0.block.0.conv1.bias', None),\n",
        "  ('first_stage_model.decoder.up.0.block.0.norm2.weight', None),\n",
        "  ('first_stage_model.decoder.up.0.block.0.norm2.bias', None),\n",
        "  ('first_stage_model.decoder.up.0.block.0.conv2.weight', (2, 3, 1, 0)),\n",
        "  ('first_stage_model.decoder.up.0.block.0.conv2.bias', None),\n",
        "  ('first_stage_model.decoder.up.0.block.0.nin_shortcut.weight', (2, 3, 1, 0)),\n",
        "  ('first_stage_model.decoder.up.0.block.0.nin_shortcut.bias', None),\n",
        "  ('first_stage_model.decoder.up.0.block.1.norm1.weight', None),\n",
        "  ('first_stage_model.decoder.up.0.block.1.norm1.bias', None),\n",
        "  ('first_stage_model.decoder.up.0.block.1.conv1.weight', (2, 3, 1, 0)),\n",
        "  ('first_stage_model.decoder.up.0.block.1.conv1.bias', None),\n",
        "  ('first_stage_model.decoder.up.0.block.1.norm2.weight', None),\n",
        "  ('first_stage_model.decoder.up.0.block.1.norm2.bias', None),\n",
        "  ('first_stage_model.decoder.up.0.block.1.conv2.weight', (2, 3, 1, 0)),\n",
        "  ('first_stage_model.decoder.up.0.block.1.conv2.bias', None),\n",
        "  ('first_stage_model.decoder.up.0.block.2.norm1.weight', None),\n",
        "  ('first_stage_model.decoder.up.0.block.2.norm1.bias', None),\n",
        "  ('first_stage_model.decoder.up.0.block.2.conv1.weight', (2, 3, 1, 0)),\n",
        "  ('first_stage_model.decoder.up.0.block.2.conv1.bias', None),\n",
        "  ('first_stage_model.decoder.up.0.block.2.norm2.weight', None),\n",
        "  ('first_stage_model.decoder.up.0.block.2.norm2.bias', None),\n",
        "  ('first_stage_model.decoder.up.0.block.2.conv2.weight', (2, 3, 1, 0)),\n",
        "  ('first_stage_model.decoder.up.0.block.2.conv2.bias', None),\n",
        "  ('first_stage_model.decoder.norm_out.weight', None),\n",
        "  ('first_stage_model.decoder.norm_out.bias', None),\n",
        "  ('first_stage_model.decoder.conv_out.weight', (2, 3, 1, 0)),\n",
        "  ('first_stage_model.decoder.conv_out.bias', None)],\n",
        " 'encoder': [('first_stage_model.encoder.conv_in.weight', (2, 3, 1, 0)),\n",
        "  ('first_stage_model.encoder.conv_in.bias', None),\n",
        "  ('first_stage_model.encoder.down.0.block.0.norm1.weight', None),\n",
        "  ('first_stage_model.encoder.down.0.block.0.norm1.bias', None),\n",
        "  ('first_stage_model.encoder.down.0.block.0.conv1.weight', (2, 3, 1, 0)),\n",
        "  ('first_stage_model.encoder.down.0.block.0.conv1.bias', None),\n",
        "  ('first_stage_model.encoder.down.0.block.0.norm2.weight', None),\n",
        "  ('first_stage_model.encoder.down.0.block.0.norm2.bias', None),\n",
        "  ('first_stage_model.encoder.down.0.block.0.conv2.weight', (2, 3, 1, 0)),\n",
        "  ('first_stage_model.encoder.down.0.block.0.conv2.bias', None),\n",
        "  ('first_stage_model.encoder.down.0.block.1.norm1.weight', None),\n",
        "  ('first_stage_model.encoder.down.0.block.1.norm1.bias', None),\n",
        "  ('first_stage_model.encoder.down.0.block.1.conv1.weight', (2, 3, 1, 0)),\n",
        "  ('first_stage_model.encoder.down.0.block.1.conv1.bias', None),\n",
        "  ('first_stage_model.encoder.down.0.block.1.norm2.weight', None),\n",
        "  ('first_stage_model.encoder.down.0.block.1.norm2.bias', None),\n",
        "  ('first_stage_model.encoder.down.0.block.1.conv2.weight', (2, 3, 1, 0)),\n",
        "  ('first_stage_model.encoder.down.0.block.1.conv2.bias', None),\n",
        "  ('first_stage_model.encoder.down.0.downsample.conv.weight', (2, 3, 1, 0)),\n",
        "  ('first_stage_model.encoder.down.0.downsample.conv.bias', None),\n",
        "  ('first_stage_model.encoder.down.1.block.0.norm1.weight', None),\n",
        "  ('first_stage_model.encoder.down.1.block.0.norm1.bias', None),\n",
        "  ('first_stage_model.encoder.down.1.block.0.conv1.weight', (2, 3, 1, 0)),\n",
        "  ('first_stage_model.encoder.down.1.block.0.conv1.bias', None),\n",
        "  ('first_stage_model.encoder.down.1.block.0.norm2.weight', None),\n",
        "  ('first_stage_model.encoder.down.1.block.0.norm2.bias', None),\n",
        "  ('first_stage_model.encoder.down.1.block.0.conv2.weight', (2, 3, 1, 0)),\n",
        "  ('first_stage_model.encoder.down.1.block.0.conv2.bias', None),\n",
        "  ('first_stage_model.encoder.down.1.block.0.nin_shortcut.weight',\n",
        "   (2, 3, 1, 0)),\n",
        "  ('first_stage_model.encoder.down.1.block.0.nin_shortcut.bias', None),\n",
        "  ('first_stage_model.encoder.down.1.block.1.norm1.weight', None),\n",
        "  ('first_stage_model.encoder.down.1.block.1.norm1.bias', None),\n",
        "  ('first_stage_model.encoder.down.1.block.1.conv1.weight', (2, 3, 1, 0)),\n",
        "  ('first_stage_model.encoder.down.1.block.1.conv1.bias', None),\n",
        "  ('first_stage_model.encoder.down.1.block.1.norm2.weight', None),\n",
        "  ('first_stage_model.encoder.down.1.block.1.norm2.bias', None),\n",
        "  ('first_stage_model.encoder.down.1.block.1.conv2.weight', (2, 3, 1, 0)),\n",
        "  ('first_stage_model.encoder.down.1.block.1.conv2.bias', None),\n",
        "  ('first_stage_model.encoder.down.1.downsample.conv.weight', (2, 3, 1, 0)),\n",
        "  ('first_stage_model.encoder.down.1.downsample.conv.bias', None),\n",
        "  ('first_stage_model.encoder.down.2.block.0.norm1.weight', None),\n",
        "  ('first_stage_model.encoder.down.2.block.0.norm1.bias', None),\n",
        "  ('first_stage_model.encoder.down.2.block.0.conv1.weight', (2, 3, 1, 0)),\n",
        "  ('first_stage_model.encoder.down.2.block.0.conv1.bias', None),\n",
        "  ('first_stage_model.encoder.down.2.block.0.norm2.weight', None),\n",
        "  ('first_stage_model.encoder.down.2.block.0.norm2.bias', None),\n",
        "  ('first_stage_model.encoder.down.2.block.0.conv2.weight', (2, 3, 1, 0)),\n",
        "  ('first_stage_model.encoder.down.2.block.0.conv2.bias', None),\n",
        "  ('first_stage_model.encoder.down.2.block.0.nin_shortcut.weight',\n",
        "   (2, 3, 1, 0)),\n",
        "  ('first_stage_model.encoder.down.2.block.0.nin_shortcut.bias', None),\n",
        "  ('first_stage_model.encoder.down.2.block.1.norm1.weight', None),\n",
        "  ('first_stage_model.encoder.down.2.block.1.norm1.bias', None),\n",
        "  ('first_stage_model.encoder.down.2.block.1.conv1.weight', (2, 3, 1, 0)),\n",
        "  ('first_stage_model.encoder.down.2.block.1.conv1.bias', None),\n",
        "  ('first_stage_model.encoder.down.2.block.1.norm2.weight', None),\n",
        "  ('first_stage_model.encoder.down.2.block.1.norm2.bias', None),\n",
        "  ('first_stage_model.encoder.down.2.block.1.conv2.weight', (2, 3, 1, 0)),\n",
        "  ('first_stage_model.encoder.down.2.block.1.conv2.bias', None),\n",
        "  ('first_stage_model.encoder.down.2.downsample.conv.weight', (2, 3, 1, 0)),\n",
        "  ('first_stage_model.encoder.down.2.downsample.conv.bias', None),\n",
        "  ('first_stage_model.encoder.down.3.block.0.norm1.weight', None),\n",
        "  ('first_stage_model.encoder.down.3.block.0.norm1.bias', None),\n",
        "  ('first_stage_model.encoder.down.3.block.0.conv1.weight', (2, 3, 1, 0)),\n",
        "  ('first_stage_model.encoder.down.3.block.0.conv1.bias', None),\n",
        "  ('first_stage_model.encoder.down.3.block.0.norm2.weight', None),\n",
        "  ('first_stage_model.encoder.down.3.block.0.norm2.bias', None),\n",
        "  ('first_stage_model.encoder.down.3.block.0.conv2.weight', (2, 3, 1, 0)),\n",
        "  ('first_stage_model.encoder.down.3.block.0.conv2.bias', None),\n",
        "  ('first_stage_model.encoder.down.3.block.1.norm1.weight', None),\n",
        "  ('first_stage_model.encoder.down.3.block.1.norm1.bias', None),\n",
        "  ('first_stage_model.encoder.down.3.block.1.conv1.weight', (2, 3, 1, 0)),\n",
        "  ('first_stage_model.encoder.down.3.block.1.conv1.bias', None),\n",
        "  ('first_stage_model.encoder.down.3.block.1.norm2.weight', None),\n",
        "  ('first_stage_model.encoder.down.3.block.1.norm2.bias', None),\n",
        "  ('first_stage_model.encoder.down.3.block.1.conv2.weight', (2, 3, 1, 0)),\n",
        "  ('first_stage_model.encoder.down.3.block.1.conv2.bias', None),\n",
        "  ('first_stage_model.encoder.mid.block_1.norm1.weight', None),\n",
        "  ('first_stage_model.encoder.mid.block_1.norm1.bias', None),\n",
        "  ('first_stage_model.encoder.mid.block_1.conv1.weight', (2, 3, 1, 0)),\n",
        "  ('first_stage_model.encoder.mid.block_1.conv1.bias', None),\n",
        "  ('first_stage_model.encoder.mid.block_1.norm2.weight', None),\n",
        "  ('first_stage_model.encoder.mid.block_1.norm2.bias', None),\n",
        "  ('first_stage_model.encoder.mid.block_1.conv2.weight', (2, 3, 1, 0)),\n",
        "  ('first_stage_model.encoder.mid.block_1.conv2.bias', None),\n",
        "  ('first_stage_model.encoder.mid.attn_1.norm.weight', None),\n",
        "  ('first_stage_model.encoder.mid.attn_1.norm.bias', None),\n",
        "  ('first_stage_model.encoder.mid.attn_1.q.weight', (2, 3, 1, 0)),\n",
        "  ('first_stage_model.encoder.mid.attn_1.q.bias', None),\n",
        "  ('first_stage_model.encoder.mid.attn_1.k.weight', (2, 3, 1, 0)),\n",
        "  ('first_stage_model.encoder.mid.attn_1.k.bias', None),\n",
        "  ('first_stage_model.encoder.mid.attn_1.v.weight', (2, 3, 1, 0)),\n",
        "  ('first_stage_model.encoder.mid.attn_1.v.bias', None),\n",
        "  ('first_stage_model.encoder.mid.attn_1.proj_out.weight', (2, 3, 1, 0)),\n",
        "  ('first_stage_model.encoder.mid.attn_1.proj_out.bias', None),\n",
        "  ('first_stage_model.encoder.mid.block_2.norm1.weight', None),\n",
        "  ('first_stage_model.encoder.mid.block_2.norm1.bias', None),\n",
        "  ('first_stage_model.encoder.mid.block_2.conv1.weight', (2, 3, 1, 0)),\n",
        "  ('first_stage_model.encoder.mid.block_2.conv1.bias', None),\n",
        "  ('first_stage_model.encoder.mid.block_2.norm2.weight', None),\n",
        "  ('first_stage_model.encoder.mid.block_2.norm2.bias', None),\n",
        "  ('first_stage_model.encoder.mid.block_2.conv2.weight', (2, 3, 1, 0)),\n",
        "  ('first_stage_model.encoder.mid.block_2.conv2.bias', None),\n",
        "  ('first_stage_model.encoder.norm_out.weight', None),\n",
        "  ('first_stage_model.encoder.norm_out.bias', None),\n",
        "  ('first_stage_model.encoder.conv_out.weight', (2, 3, 1, 0)),\n",
        "  ('first_stage_model.encoder.conv_out.bias', None),\n",
        "  ('first_stage_model.quant_conv.weight', (2, 3, 1, 0)),\n",
        "  ('first_stage_model.quant_conv.bias', None)]}\n",
        "\n",
        "\n",
        "_UNCONDITIONAL_TOKENS = [\n",
        "    49406,\n",
        "    49407,\n",
        "    49407,\n",
        "    49407,\n",
        "    49407,\n",
        "    49407,\n",
        "    49407,\n",
        "    49407,\n",
        "    49407,\n",
        "    49407,\n",
        "    49407,\n",
        "    49407,\n",
        "    49407,\n",
        "    49407,\n",
        "    49407,\n",
        "    49407,\n",
        "    49407,\n",
        "    49407,\n",
        "    49407,\n",
        "    49407,\n",
        "    49407,\n",
        "    49407,\n",
        "    49407,\n",
        "    49407,\n",
        "    49407,\n",
        "    49407,\n",
        "    49407,\n",
        "    49407,\n",
        "    49407,\n",
        "    49407,\n",
        "    49407,\n",
        "    49407,\n",
        "    49407,\n",
        "    49407,\n",
        "    49407,\n",
        "    49407,\n",
        "    49407,\n",
        "    49407,\n",
        "    49407,\n",
        "    49407,\n",
        "    49407,\n",
        "    49407,\n",
        "    49407,\n",
        "    49407,\n",
        "    49407,\n",
        "    49407,\n",
        "    49407,\n",
        "    49407,\n",
        "    49407,\n",
        "    49407,\n",
        "    49407,\n",
        "    49407,\n",
        "    49407,\n",
        "    49407,\n",
        "    49407,\n",
        "    49407,\n",
        "    49407,\n",
        "    49407,\n",
        "    49407,\n",
        "    49407,\n",
        "    49407,\n",
        "    49407,\n",
        "    49407,\n",
        "    49407,\n",
        "    49407,\n",
        "    49407,\n",
        "    49407,\n",
        "    49407,\n",
        "    49407,\n",
        "    49407,\n",
        "    49407,\n",
        "    49407,\n",
        "    49407,\n",
        "    49407,\n",
        "    49407,\n",
        "    49407,\n",
        "    49407,\n",
        "]\n",
        "_ALPHAS_CUMPROD = [\n",
        "    0.99915,\n",
        "    0.998296,\n",
        "    0.9974381,\n",
        "    0.9965762,\n",
        "    0.99571025,\n",
        "    0.9948404,\n",
        "    0.9939665,\n",
        "    0.9930887,\n",
        "    0.9922069,\n",
        "    0.9913211,\n",
        "    0.9904313,\n",
        "    0.98953754,\n",
        "    0.9886398,\n",
        "    0.9877381,\n",
        "    0.9868324,\n",
        "    0.98592263,\n",
        "    0.98500896,\n",
        "    0.9840913,\n",
        "    0.9831696,\n",
        "    0.982244,\n",
        "    0.98131436,\n",
        "    0.9803808,\n",
        "    0.97944313,\n",
        "    0.97850156,\n",
        "    0.977556,\n",
        "    0.9766064,\n",
        "    0.97565293,\n",
        "    0.9746954,\n",
        "    0.9737339,\n",
        "    0.9727684,\n",
        "    0.97179896,\n",
        "    0.97082555,\n",
        "    0.96984816,\n",
        "    0.96886677,\n",
        "    0.9678814,\n",
        "    0.96689206,\n",
        "    0.96589875,\n",
        "    0.9649015,\n",
        "    0.96390027,\n",
        "    0.9628951,\n",
        "    0.9618859,\n",
        "    0.96087277,\n",
        "    0.95985574,\n",
        "    0.95883465,\n",
        "    0.9578097,\n",
        "    0.95678073,\n",
        "    0.95574784,\n",
        "    0.954711,\n",
        "    0.95367026,\n",
        "    0.9526256,\n",
        "    0.9515769,\n",
        "    0.95052433,\n",
        "    0.94946784,\n",
        "    0.94840735,\n",
        "    0.947343,\n",
        "    0.94627476,\n",
        "    0.9452025,\n",
        "    0.9441264,\n",
        "    0.9430464,\n",
        "    0.9419625,\n",
        "    0.9408747,\n",
        "    0.939783,\n",
        "    0.9386874,\n",
        "    0.93758786,\n",
        "    0.9364845,\n",
        "    0.93537724,\n",
        "    0.9342661,\n",
        "    0.9331511,\n",
        "    0.9320323,\n",
        "    0.9309096,\n",
        "    0.929783,\n",
        "    0.9286526,\n",
        "    0.9275183,\n",
        "    0.9263802,\n",
        "    0.92523825,\n",
        "    0.92409253,\n",
        "    0.92294294,\n",
        "    0.9217895,\n",
        "    0.92063236,\n",
        "    0.9194713,\n",
        "    0.9183065,\n",
        "    0.9171379,\n",
        "    0.91596556,\n",
        "    0.9147894,\n",
        "    0.9136095,\n",
        "    0.91242576,\n",
        "    0.9112383,\n",
        "    0.9100471,\n",
        "    0.9088522,\n",
        "    0.9076535,\n",
        "    0.9064511,\n",
        "    0.90524495,\n",
        "    0.9040351,\n",
        "    0.90282154,\n",
        "    0.9016043,\n",
        "    0.90038335,\n",
        "    0.8991587,\n",
        "    0.8979304,\n",
        "    0.8966984,\n",
        "    0.89546275,\n",
        "    0.89422345,\n",
        "    0.8929805,\n",
        "    0.89173394,\n",
        "    0.89048374,\n",
        "    0.88922995,\n",
        "    0.8879725,\n",
        "    0.8867115,\n",
        "    0.88544685,\n",
        "    0.88417864,\n",
        "    0.88290685,\n",
        "    0.8816315,\n",
        "    0.88035256,\n",
        "    0.8790701,\n",
        "    0.87778413,\n",
        "    0.8764946,\n",
        "    0.8752016,\n",
        "    0.873905,\n",
        "    0.87260497,\n",
        "    0.8713014,\n",
        "    0.8699944,\n",
        "    0.86868393,\n",
        "    0.86737,\n",
        "    0.8660526,\n",
        "    0.8647318,\n",
        "    0.86340755,\n",
        "    0.8620799,\n",
        "    0.8607488,\n",
        "    0.85941434,\n",
        "    0.8580765,\n",
        "    0.8567353,\n",
        "    0.8553907,\n",
        "    0.8540428,\n",
        "    0.85269153,\n",
        "    0.85133696,\n",
        "    0.84997904,\n",
        "    0.84861785,\n",
        "    0.8472533,\n",
        "    0.8458856,\n",
        "    0.8445145,\n",
        "    0.84314024,\n",
        "    0.84176266,\n",
        "    0.8403819,\n",
        "    0.8389979,\n",
        "    0.8376107,\n",
        "    0.8362203,\n",
        "    0.83482677,\n",
        "    0.83343,\n",
        "    0.8320301,\n",
        "    0.8306271,\n",
        "    0.8292209,\n",
        "    0.82781166,\n",
        "    0.82639927,\n",
        "    0.8249838,\n",
        "    0.82356524,\n",
        "    0.8221436,\n",
        "    0.82071894,\n",
        "    0.81929123,\n",
        "    0.81786054,\n",
        "    0.8164268,\n",
        "    0.8149901,\n",
        "    0.8135504,\n",
        "    0.81210774,\n",
        "    0.81066215,\n",
        "    0.8092136,\n",
        "    0.8077621,\n",
        "    0.80630773,\n",
        "    0.80485046,\n",
        "    0.8033903,\n",
        "    0.80192727,\n",
        "    0.8004614,\n",
        "    0.79899275,\n",
        "    0.79752123,\n",
        "    0.7960469,\n",
        "    0.7945698,\n",
        "    0.7930899,\n",
        "    0.79160726,\n",
        "    0.7901219,\n",
        "    0.7886338,\n",
        "    0.787143,\n",
        "    0.7856495,\n",
        "    0.7841533,\n",
        "    0.78265446,\n",
        "    0.78115296,\n",
        "    0.7796488,\n",
        "    0.77814204,\n",
        "    0.7766327,\n",
        "    0.7751208,\n",
        "    0.7736063,\n",
        "    0.77208924,\n",
        "    0.7705697,\n",
        "    0.7690476,\n",
        "    0.767523,\n",
        "    0.7659959,\n",
        "    0.7644664,\n",
        "    0.76293445,\n",
        "    0.7614,\n",
        "    0.7598632,\n",
        "    0.75832397,\n",
        "    0.75678235,\n",
        "    0.75523835,\n",
        "    0.75369203,\n",
        "    0.7521434,\n",
        "    0.75059247,\n",
        "    0.7490392,\n",
        "    0.7474837,\n",
        "    0.7459259,\n",
        "    0.7443659,\n",
        "    0.74280363,\n",
        "    0.7412392,\n",
        "    0.7396726,\n",
        "    0.7381038,\n",
        "    0.73653287,\n",
        "    0.7349598,\n",
        "    0.7333846,\n",
        "    0.73180735,\n",
        "    0.730228,\n",
        "    0.7286466,\n",
        "    0.7270631,\n",
        "    0.7254777,\n",
        "    0.72389024,\n",
        "    0.72230077,\n",
        "    0.7207094,\n",
        "    0.71911603,\n",
        "    0.7175208,\n",
        "    0.7159236,\n",
        "    0.71432453,\n",
        "    0.7127236,\n",
        "    0.71112084,\n",
        "    0.7095162,\n",
        "    0.7079098,\n",
        "    0.7063016,\n",
        "    0.70469165,\n",
        "    0.70307994,\n",
        "    0.7014665,\n",
        "    0.69985133,\n",
        "    0.6982345,\n",
        "    0.696616,\n",
        "    0.6949958,\n",
        "    0.69337404,\n",
        "    0.69175065,\n",
        "    0.69012564,\n",
        "    0.6884991,\n",
        "    0.68687093,\n",
        "    0.6852413,\n",
        "    0.68361014,\n",
        "    0.6819775,\n",
        "    0.6803434,\n",
        "    0.67870784,\n",
        "    0.6770708,\n",
        "    0.6754324,\n",
        "    0.6737926,\n",
        "    0.67215145,\n",
        "    0.670509,\n",
        "    0.66886514,\n",
        "    0.66722,\n",
        "    0.6655736,\n",
        "    0.66392595,\n",
        "    0.662277,\n",
        "    0.6606269,\n",
        "    0.65897554,\n",
        "    0.657323,\n",
        "    0.65566933,\n",
        "    0.6540145,\n",
        "    0.6523586,\n",
        "    0.6507016,\n",
        "    0.6490435,\n",
        "    0.64738435,\n",
        "    0.6457241,\n",
        "    0.64406294,\n",
        "    0.6424008,\n",
        "    0.64073765,\n",
        "    0.63907355,\n",
        "    0.63740855,\n",
        "    0.6357426,\n",
        "    0.6340758,\n",
        "    0.6324082,\n",
        "    0.6307397,\n",
        "    0.6290704,\n",
        "    0.6274003,\n",
        "    0.6257294,\n",
        "    0.62405777,\n",
        "    0.6223854,\n",
        "    0.62071234,\n",
        "    0.6190386,\n",
        "    0.61736417,\n",
        "    0.6156891,\n",
        "    0.61401343,\n",
        "    0.6123372,\n",
        "    0.6106603,\n",
        "    0.6089829,\n",
        "    0.607305,\n",
        "    0.6056265,\n",
        "    0.6039476,\n",
        "    0.60226816,\n",
        "    0.6005883,\n",
        "    0.598908,\n",
        "    0.59722733,\n",
        "    0.5955463,\n",
        "    0.59386486,\n",
        "    0.5921831,\n",
        "    0.59050107,\n",
        "    0.5888187,\n",
        "    0.5871361,\n",
        "    0.5854532,\n",
        "    0.5837701,\n",
        "    0.5820868,\n",
        "    0.5804033,\n",
        "    0.5787197,\n",
        "    0.5770359,\n",
        "    0.575352,\n",
        "    0.57366806,\n",
        "    0.571984,\n",
        "    0.5702999,\n",
        "    0.5686158,\n",
        "    0.56693166,\n",
        "    0.56524754,\n",
        "    0.5635635,\n",
        "    0.5618795,\n",
        "    0.56019557,\n",
        "    0.5585118,\n",
        "    0.5568281,\n",
        "    0.55514455,\n",
        "    0.5534612,\n",
        "    0.551778,\n",
        "    0.5500951,\n",
        "    0.5484124,\n",
        "    0.54673,\n",
        "    0.5450478,\n",
        "    0.54336596,\n",
        "    0.54168445,\n",
        "    0.54000324,\n",
        "    0.53832245,\n",
        "    0.5366421,\n",
        "    0.53496206,\n",
        "    0.5332825,\n",
        "    0.53160346,\n",
        "    0.5299248,\n",
        "    0.52824676,\n",
        "    0.5265692,\n",
        "    0.52489215,\n",
        "    0.5232157,\n",
        "    0.5215398,\n",
        "    0.51986456,\n",
        "    0.51818997,\n",
        "    0.51651603,\n",
        "    0.51484275,\n",
        "    0.5131702,\n",
        "    0.5114983,\n",
        "    0.5098272,\n",
        "    0.50815684,\n",
        "    0.5064873,\n",
        "    0.50481856,\n",
        "    0.50315064,\n",
        "    0.50148356,\n",
        "    0.4998174,\n",
        "    0.4981521,\n",
        "    0.49648774,\n",
        "    0.49482432,\n",
        "    0.49316183,\n",
        "    0.49150035,\n",
        "    0.48983985,\n",
        "    0.4881804,\n",
        "    0.486522,\n",
        "    0.48486462,\n",
        "    0.4832084,\n",
        "    0.48155323,\n",
        "    0.4798992,\n",
        "    0.47824633,\n",
        "    0.47659463,\n",
        "    0.4749441,\n",
        "    0.47329482,\n",
        "    0.4716468,\n",
        "    0.47,\n",
        "    0.46835446,\n",
        "    0.46671024,\n",
        "    0.46506736,\n",
        "    0.4634258,\n",
        "    0.46178558,\n",
        "    0.46014675,\n",
        "    0.45850933,\n",
        "    0.45687333,\n",
        "    0.45523876,\n",
        "    0.45360568,\n",
        "    0.45197406,\n",
        "    0.45034397,\n",
        "    0.44871536,\n",
        "    0.44708833,\n",
        "    0.44546285,\n",
        "    0.44383895,\n",
        "    0.44221666,\n",
        "    0.440596,\n",
        "    0.43897697,\n",
        "    0.43735963,\n",
        "    0.43574396,\n",
        "    0.43412998,\n",
        "    0.43251774,\n",
        "    0.43090722,\n",
        "    0.4292985,\n",
        "    0.42769152,\n",
        "    0.42608637,\n",
        "    0.42448303,\n",
        "    0.4228815,\n",
        "    0.42128187,\n",
        "    0.4196841,\n",
        "    0.41808826,\n",
        "    0.4164943,\n",
        "    0.4149023,\n",
        "    0.41331223,\n",
        "    0.41172415,\n",
        "    0.41013804,\n",
        "    0.40855396,\n",
        "    0.4069719,\n",
        "    0.4053919,\n",
        "    0.40381396,\n",
        "    0.4022381,\n",
        "    0.40066436,\n",
        "    0.39909273,\n",
        "    0.39752322,\n",
        "    0.3959559,\n",
        "    0.39439073,\n",
        "    0.39282778,\n",
        "    0.39126703,\n",
        "    0.3897085,\n",
        "    0.3881522,\n",
        "    0.3865982,\n",
        "    0.38504648,\n",
        "    0.38349706,\n",
        "    0.38194993,\n",
        "    0.38040516,\n",
        "    0.37886274,\n",
        "    0.37732267,\n",
        "    0.375785,\n",
        "    0.37424973,\n",
        "    0.37271687,\n",
        "    0.37118647,\n",
        "    0.36965853,\n",
        "    0.36813304,\n",
        "    0.36661002,\n",
        "    0.36508954,\n",
        "    0.36357155,\n",
        "    0.3620561,\n",
        "    0.36054322,\n",
        "    0.3590329,\n",
        "    0.35752517,\n",
        "    0.35602003,\n",
        "    0.35451752,\n",
        "    0.35301763,\n",
        "    0.3515204,\n",
        "    0.3500258,\n",
        "    0.3485339,\n",
        "    0.3470447,\n",
        "    0.34555823,\n",
        "    0.34407446,\n",
        "    0.34259343,\n",
        "    0.34111515,\n",
        "    0.33963963,\n",
        "    0.33816692,\n",
        "    0.336697,\n",
        "    0.3352299,\n",
        "    0.33376563,\n",
        "    0.3323042,\n",
        "    0.33084565,\n",
        "    0.32938993,\n",
        "    0.32793713,\n",
        "    0.3264872,\n",
        "    0.32504022,\n",
        "    0.32359615,\n",
        "    0.32215503,\n",
        "    0.32071686,\n",
        "    0.31928164,\n",
        "    0.31784943,\n",
        "    0.3164202,\n",
        "    0.314994,\n",
        "    0.3135708,\n",
        "    0.31215066,\n",
        "    0.31073356,\n",
        "    0.3093195,\n",
        "    0.30790854,\n",
        "    0.30650064,\n",
        "    0.30509588,\n",
        "    0.30369422,\n",
        "    0.30229566,\n",
        "    0.30090025,\n",
        "    0.299508,\n",
        "    0.2981189,\n",
        "    0.29673296,\n",
        "    0.29535022,\n",
        "    0.2939707,\n",
        "    0.29259437,\n",
        "    0.29122123,\n",
        "    0.28985137,\n",
        "    0.28848472,\n",
        "    0.28712133,\n",
        "    0.2857612,\n",
        "    0.28440437,\n",
        "    0.2830508,\n",
        "    0.28170055,\n",
        "    0.2803536,\n",
        "    0.27900997,\n",
        "    0.27766964,\n",
        "    0.27633268,\n",
        "    0.27499905,\n",
        "    0.2736688,\n",
        "    0.27234194,\n",
        "    0.27101842,\n",
        "    0.2696983,\n",
        "    0.26838157,\n",
        "    0.26706827,\n",
        "    0.26575837,\n",
        "    0.26445192,\n",
        "    0.26314887,\n",
        "    0.2618493,\n",
        "    0.26055318,\n",
        "    0.2592605,\n",
        "    0.25797132,\n",
        "    0.2566856,\n",
        "    0.2554034,\n",
        "    0.25412467,\n",
        "    0.25284946,\n",
        "    0.25157773,\n",
        "    0.2503096,\n",
        "    0.24904492,\n",
        "    0.24778382,\n",
        "    0.24652626,\n",
        "    0.24527225,\n",
        "    0.2440218,\n",
        "    0.24277493,\n",
        "    0.24153163,\n",
        "    0.24029191,\n",
        "    0.23905578,\n",
        "    0.23782326,\n",
        "    0.23659433,\n",
        "    0.23536903,\n",
        "    0.23414734,\n",
        "    0.23292927,\n",
        "    0.23171483,\n",
        "    0.23050404,\n",
        "    0.22929688,\n",
        "    0.22809339,\n",
        "    0.22689353,\n",
        "    0.22569734,\n",
        "    0.22450483,\n",
        "    0.22331597,\n",
        "    0.2221308,\n",
        "    0.22094932,\n",
        "    0.21977153,\n",
        "    0.21859743,\n",
        "    0.21742703,\n",
        "    0.21626033,\n",
        "    0.21509734,\n",
        "    0.21393807,\n",
        "    0.21278252,\n",
        "    0.21163069,\n",
        "    0.21048258,\n",
        "    0.20933822,\n",
        "    0.20819758,\n",
        "    0.2070607,\n",
        "    0.20592754,\n",
        "    0.20479813,\n",
        "    0.20367248,\n",
        "    0.20255059,\n",
        "    0.20143245,\n",
        "    0.20031808,\n",
        "    0.19920748,\n",
        "    0.19810064,\n",
        "    0.19699757,\n",
        "    0.19589828,\n",
        "    0.19480278,\n",
        "    0.19371104,\n",
        "    0.1926231,\n",
        "    0.19153893,\n",
        "    0.19045855,\n",
        "    0.18938197,\n",
        "    0.18830918,\n",
        "    0.18724018,\n",
        "    0.18617497,\n",
        "    0.18511358,\n",
        "    0.18405597,\n",
        "    0.18300217,\n",
        "    0.18195218,\n",
        "    0.18090598,\n",
        "    0.1798636,\n",
        "    0.17882504,\n",
        "    0.17779027,\n",
        "    0.1767593,\n",
        "    0.17573217,\n",
        "    0.17470883,\n",
        "    0.1736893,\n",
        "    0.1726736,\n",
        "    0.1716617,\n",
        "    0.17065361,\n",
        "    0.16964935,\n",
        "    0.1686489,\n",
        "    0.16765225,\n",
        "    0.16665943,\n",
        "    0.16567042,\n",
        "    0.16468522,\n",
        "    0.16370384,\n",
        "    0.16272627,\n",
        "    0.16175252,\n",
        "    0.16078258,\n",
        "    0.15981644,\n",
        "    0.15885411,\n",
        "    0.1578956,\n",
        "    0.15694089,\n",
        "    0.15599,\n",
        "    0.15504292,\n",
        "    0.15409963,\n",
        "    0.15316014,\n",
        "    0.15222447,\n",
        "    0.15129258,\n",
        "    0.1503645,\n",
        "    0.14944021,\n",
        "    0.14851972,\n",
        "    0.14760303,\n",
        "    0.14669013,\n",
        "    0.14578101,\n",
        "    0.14487568,\n",
        "    0.14397413,\n",
        "    0.14307636,\n",
        "    0.14218238,\n",
        "    0.14129217,\n",
        "    0.14040573,\n",
        "    0.13952307,\n",
        "    0.13864417,\n",
        "    0.13776903,\n",
        "    0.13689767,\n",
        "    0.13603005,\n",
        "    0.13516618,\n",
        "    0.13430607,\n",
        "    0.13344972,\n",
        "    0.1325971,\n",
        "    0.13174823,\n",
        "    0.1309031,\n",
        "    0.13006169,\n",
        "    0.12922402,\n",
        "    0.12839006,\n",
        "    0.12755983,\n",
        "    0.12673332,\n",
        "    0.12591052,\n",
        "    0.12509143,\n",
        "    0.12427604,\n",
        "    0.12346435,\n",
        "    0.12265636,\n",
        "    0.121852055,\n",
        "    0.12105144,\n",
        "    0.1202545,\n",
        "    0.11946124,\n",
        "    0.11867165,\n",
        "    0.11788572,\n",
        "    0.11710346,\n",
        "    0.11632485,\n",
        "    0.115549885,\n",
        "    0.11477857,\n",
        "    0.11401089,\n",
        "    0.11324684,\n",
        "    0.11248643,\n",
        "    0.11172963,\n",
        "    0.11097645,\n",
        "    0.110226884,\n",
        "    0.10948092,\n",
        "    0.10873855,\n",
        "    0.10799977,\n",
        "    0.107264586,\n",
        "    0.106532976,\n",
        "    0.105804935,\n",
        "    0.10508047,\n",
        "    0.10435956,\n",
        "    0.1036422,\n",
        "    0.10292839,\n",
        "    0.10221813,\n",
        "    0.1015114,\n",
        "    0.10080819,\n",
        "    0.100108504,\n",
        "    0.09941233,\n",
        "    0.098719664,\n",
        "    0.0980305,\n",
        "    0.09734483,\n",
        "    0.09666264,\n",
        "    0.09598393,\n",
        "    0.095308684,\n",
        "    0.09463691,\n",
        "    0.093968585,\n",
        "    0.09330372,\n",
        "    0.092642285,\n",
        "    0.09198428,\n",
        "    0.09132971,\n",
        "    0.09067855,\n",
        "    0.090030804,\n",
        "    0.089386456,\n",
        "    0.088745505,\n",
        "    0.088107936,\n",
        "    0.08747375,\n",
        "    0.08684293,\n",
        "    0.08621547,\n",
        "    0.085591376,\n",
        "    0.084970616,\n",
        "    0.08435319,\n",
        "    0.0837391,\n",
        "    0.08312833,\n",
        "    0.08252087,\n",
        "    0.08191671,\n",
        "    0.08131585,\n",
        "    0.08071827,\n",
        "    0.080123976,\n",
        "    0.07953294,\n",
        "    0.078945175,\n",
        "    0.078360654,\n",
        "    0.077779375,\n",
        "    0.07720133,\n",
        "    0.07662651,\n",
        "    0.07605491,\n",
        "    0.07548651,\n",
        "    0.07492131,\n",
        "    0.0743593,\n",
        "    0.07380046,\n",
        "    0.073244795,\n",
        "    0.07269229,\n",
        "    0.07214294,\n",
        "    0.07159673,\n",
        "    0.07105365,\n",
        "    0.070513695,\n",
        "    0.06997685,\n",
        "    0.069443114,\n",
        "    0.06891247,\n",
        "    0.06838491,\n",
        "    0.067860425,\n",
        "    0.06733901,\n",
        "    0.066820644,\n",
        "    0.06630533,\n",
        "    0.06579305,\n",
        "    0.0652838,\n",
        "    0.06477757,\n",
        "    0.06427433,\n",
        "    0.0637741,\n",
        "    0.063276865,\n",
        "    0.06278259,\n",
        "    0.062291294,\n",
        "    0.061802953,\n",
        "    0.06131756,\n",
        "    0.0608351,\n",
        "    0.060355574,\n",
        "    0.05987896,\n",
        "    0.059405252,\n",
        "    0.058934443,\n",
        "    0.05846652,\n",
        "    0.058001474,\n",
        "    0.057539295,\n",
        "    0.05707997,\n",
        "    0.056623492,\n",
        "    0.05616985,\n",
        "    0.05571903,\n",
        "    0.055271026,\n",
        "    0.054825824,\n",
        "    0.05438342,\n",
        "    0.053943794,\n",
        "    0.053506944,\n",
        "    0.05307286,\n",
        "    0.052641522,\n",
        "    0.052212927,\n",
        "    0.051787063,\n",
        "    0.051363923,\n",
        "    0.05094349,\n",
        "    0.050525755,\n",
        "    0.05011071,\n",
        "    0.04969834,\n",
        "    0.049288645,\n",
        "    0.0488816,\n",
        "    0.048477206,\n",
        "    0.048075445,\n",
        "    0.04767631,\n",
        "    0.047279786,\n",
        "    0.04688587,\n",
        "    0.046494544,\n",
        "    0.046105802,\n",
        "    0.04571963,\n",
        "    0.04533602,\n",
        "    0.04495496,\n",
        "    0.04457644,\n",
        "    0.044200446,\n",
        "    0.04382697,\n",
        "    0.043456003,\n",
        "    0.043087535,\n",
        "    0.042721547,\n",
        "    0.042358037,\n",
        "    0.04199699,\n",
        "    0.041638397,\n",
        "    0.041282244,\n",
        "    0.040928524,\n",
        "    0.040577225,\n",
        "    0.040228333,\n",
        "    0.039881844,\n",
        "    0.039537743,\n",
        "    0.039196018,\n",
        "    0.038856663,\n",
        "    0.038519662,\n",
        "    0.038185004,\n",
        "    0.037852682,\n",
        "    0.037522685,\n",
        "    0.037195,\n",
        "    0.036869615,\n",
        "    0.036546525,\n",
        "    0.036225714,\n",
        "    0.03590717,\n",
        "    0.035590887,\n",
        "    0.035276853,\n",
        "    0.034965057,\n",
        "    0.034655485,\n",
        "    0.03434813,\n",
        "    0.03404298,\n",
        "    0.033740025,\n",
        "    0.033439253,\n",
        "    0.033140652,\n",
        "    0.032844216,\n",
        "    0.03254993,\n",
        "    0.032257784,\n",
        "    0.03196777,\n",
        "    0.031679876,\n",
        "    0.031394087,\n",
        "    0.031110398,\n",
        "    0.030828796,\n",
        "    0.030549273,\n",
        "    0.030271813,\n",
        "    0.02999641,\n",
        "    0.029723052,\n",
        "    0.029451728,\n",
        "    0.029182427,\n",
        "    0.02891514,\n",
        "    0.028649855,\n",
        "    0.028386563,\n",
        "    0.028125253,\n",
        "    0.02786591,\n",
        "    0.027608532,\n",
        "    0.027353102,\n",
        "    0.027099613,\n",
        "    0.026848052,\n",
        "    0.026598409,\n",
        "    0.026350675,\n",
        "    0.02610484,\n",
        "    0.02586089,\n",
        "    0.02561882,\n",
        "    0.025378617,\n",
        "    0.025140269,\n",
        "    0.024903767,\n",
        "    0.0246691,\n",
        "    0.02443626,\n",
        "    0.024205236,\n",
        "    0.023976017,\n",
        "    0.023748592,\n",
        "    0.023522953,\n",
        "    0.023299087,\n",
        "    0.023076987,\n",
        "    0.022856642,\n",
        "    0.02263804,\n",
        "    0.022421172,\n",
        "    0.022206029,\n",
        "    0.0219926,\n",
        "    0.021780876,\n",
        "    0.021570845,\n",
        "    0.021362498,\n",
        "    0.021155827,\n",
        "    0.020950818,\n",
        "    0.020747466,\n",
        "    0.020545758,\n",
        "    0.020345684,\n",
        "    0.020147236,\n",
        "    0.019950403,\n",
        "    0.019755175,\n",
        "    0.019561544,\n",
        "    0.019369498,\n",
        "    0.019179028,\n",
        "    0.018990126,\n",
        "    0.01880278,\n",
        "    0.018616982,\n",
        "    0.018432721,\n",
        "    0.01824999,\n",
        "    0.018068777,\n",
        "    0.017889075,\n",
        "    0.017710872,\n",
        "    0.01753416,\n",
        "    0.017358929,\n",
        "    0.017185168,\n",
        "    0.017012872,\n",
        "    0.016842028,\n",
        "    0.016672628,\n",
        "    0.016504662,\n",
        "    0.016338123,\n",
        "    0.016173,\n",
        "    0.016009282,\n",
        "    0.015846964,\n",
        "    0.015686033,\n",
        "    0.015526483,\n",
        "    0.015368304,\n",
        "    0.015211486,\n",
        "    0.0150560215,\n",
        "    0.014901901,\n",
        "    0.014749114,\n",
        "    0.014597654,\n",
        "    0.014447511,\n",
        "    0.0142986765,\n",
        "    0.014151142,\n",
        "    0.014004898,\n",
        "    0.013859936,\n",
        "    0.013716248,\n",
        "    0.0135738235,\n",
        "    0.013432656,\n",
        "    0.013292736,\n",
        "    0.013154055,\n",
        "    0.013016605,\n",
        "    0.012880377,\n",
        "    0.012745362,\n",
        "    0.012611552,\n",
        "    0.012478939,\n",
        "    0.012347515,\n",
        "    0.01221727,\n",
        "    0.012088198,\n",
        "    0.0119602885,\n",
        "    0.0118335355,\n",
        "    0.011707929,\n",
        "    0.011583461,\n",
        "    0.011460125,\n",
        "    0.011337912,\n",
        "    0.011216813,\n",
        "    0.011096821,\n",
        "    0.010977928,\n",
        "    0.0108601255,\n",
        "    0.010743406,\n",
        "    0.010627762,\n",
        "    0.0105131855,\n",
        "    0.010399668,\n",
        "    0.010287202,\n",
        "    0.01017578,\n",
        "    0.010065395,\n",
        "    0.009956039,\n",
        "    0.009847702,\n",
        "    0.009740381,\n",
        "    0.0096340645,\n",
        "    0.009528747,\n",
        "    0.009424419,\n",
        "    0.009321076,\n",
        "    0.009218709,\n",
        "    0.00911731,\n",
        "    0.009016872,\n",
        "    0.008917389,\n",
        "    0.008818853,\n",
        "    0.008721256,\n",
        "    0.008624591,\n",
        "    0.008528852,\n",
        "    0.00843403,\n",
        "    0.00834012,\n",
        "    0.008247114,\n",
        "    0.008155004,\n",
        "    0.008063785,\n",
        "    0.007973449,\n",
        "    0.007883989,\n",
        "    0.007795398,\n",
        "    0.0077076694,\n",
        "    0.0076207966,\n",
        "    0.0075347726,\n",
        "    0.007449591,\n",
        "    0.0073652444,\n",
        "    0.007281727,\n",
        "    0.0071990318,\n",
        "    0.007117152,\n",
        "    0.0070360815,\n",
        "    0.0069558136,\n",
        "    0.0068763415,\n",
        "    0.006797659,\n",
        "    0.00671976,\n",
        "    0.0066426382,\n",
        "    0.0065662866,\n",
        "    0.006490699,\n",
        "    0.0064158696,\n",
        "    0.006341792,\n",
        "    0.00626846,\n",
        "    0.0061958674,\n",
        "    0.0061240084,\n",
        "    0.0060528764,\n",
        "    0.0059824656,\n",
        "    0.0059127696,\n",
        "    0.0058437833,\n",
        "    0.0057755,\n",
        "    0.0057079145,\n",
        "    0.00564102,\n",
        "    0.0055748112,\n",
        "    0.0055092825,\n",
        "    0.005444428,\n",
        "    0.005380241,\n",
        "    0.0053167176,\n",
        "    0.005253851,\n",
        "    0.005191636,\n",
        "    0.005130066,\n",
        "    0.0050691366,\n",
        "    0.0050088423,\n",
        "    0.0049491767,\n",
        "    0.004890135,\n",
        "    0.0048317118,\n",
        "    0.004773902,\n",
        "    0.004716699,\n",
        "    0.0046600983,\n",
        "]\n"
      ],
      "metadata": {
        "id": "5PsI4lYgKwCK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PaddedConv2D(keras.layers.Layer):\n",
        "    def __init__(self, channels, kernel_size, padding=0, stride=1):\n",
        "        super().__init__()\n",
        "        self.padding2d = keras.layers.ZeroPadding2D((padding, padding))\n",
        "        self.conv2d = keras.layers.Conv2D(\n",
        "            channels, kernel_size, strides=(stride, stride)\n",
        "        )\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.padding2d(x)\n",
        "        return self.conv2d(x)\n",
        "\n",
        "\n",
        "class GEGLU(keras.layers.Layer):\n",
        "    def __init__(self, dim_out):\n",
        "        super().__init__()\n",
        "        self.proj = keras.layers.Dense(dim_out * 2)\n",
        "        self.dim_out = dim_out\n",
        "\n",
        "    def call(self, x):\n",
        "        xp = self.proj(x)\n",
        "        x, gate = xp[..., : self.dim_out], xp[..., self.dim_out :]\n",
        "        return x * gelu(gate)\n",
        "\n",
        "\n",
        "def gelu(x):\n",
        "    tanh_res = keras.activations.tanh(x * 0.7978845608 * (1 + 0.044715 * (x**2)))\n",
        "    return 0.5 * x * (1 + tanh_res)\n",
        "\n",
        "\n",
        "def quick_gelu(x):\n",
        "    return x * tf.sigmoid(x * 1.702)\n",
        "\n",
        "\n",
        "def apply_seq(x, layers):\n",
        "    for l in layers:\n",
        "        x = l(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def td_dot(a, b):\n",
        "    aa = tf.reshape(a, (-1, a.shape[2], a.shape[3]))\n",
        "    bb = tf.reshape(b, (-1, b.shape[2], b.shape[3]))\n",
        "    cc = keras.backend.batch_dot(aa, bb)\n",
        "    return tf.reshape(cc, (-1, a.shape[1], cc.shape[1], cc.shape[2]))\n"
      ],
      "metadata": {
        "id": "6RbSXN60Enee"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#AUTO ENCODER\n"
      ],
      "metadata": {
        "id": "dgpRvYc-FVly"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Attention Block"
      ],
      "metadata": {
        "id": "XGp39EsTFNfe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionBlock(keras.layers.Layer):\n",
        "    def __init__(self, channels):\n",
        "        super().__init__()\n",
        "        self.norm = tfa.layers.GroupNormalization(epsilon=1e-5)\n",
        "        self.q = PaddedConv2D(channels, 1)\n",
        "        self.k = PaddedConv2D(channels, 1)\n",
        "        self.v = PaddedConv2D(channels, 1)\n",
        "        self.proj_out = PaddedConv2D(channels, 1)\n",
        "\n",
        "    def call(self, x):\n",
        "        h_ = self.norm(x)\n",
        "        q, k, v = self.q(h_), self.k(h_), self.v(h_)\n",
        "\n",
        "        # Compute attention\n",
        "        b, h, w, c = q.shape\n",
        "        q = tf.reshape(q, (-1, h * w, c))  # b,hw,c\n",
        "        k = keras.layers.Permute((3, 1, 2))(k)\n",
        "        k = tf.reshape(k, (-1, c, h * w))  # b,c,hw\n",
        "        w_ = q @ k\n",
        "        w_ = w_ * (c ** (-0.5))\n",
        "        w_ = keras.activations.softmax(w_)\n",
        "\n",
        "        # Attend to values\n",
        "        v = keras.layers.Permute((3, 1, 2))(v)\n",
        "        v = tf.reshape(v, (-1, c, h * w))\n",
        "        w_ = keras.layers.Permute((2, 1))(w_)\n",
        "        h_ = v @ w_\n",
        "        h_ = keras.layers.Permute((2, 1))(h_)\n",
        "        h_ = tf.reshape(h_, (-1, h, w, c))\n",
        "        return x + self.proj_out(h_)\n",
        "\n"
      ],
      "metadata": {
        "id": "SKyla0VTFTUi"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##ResnetBlock"
      ],
      "metadata": {
        "id": "vA2KhkpTFGbY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResnetBlock(keras.layers.Layer):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.norm1 = tfa.layers.GroupNormalization(epsilon=1e-5)\n",
        "        self.conv1 = PaddedConv2D(out_channels, 3, padding=1)\n",
        "        self.norm2 = keras.layers.GroupNormalization(epsilon=1e-5)\n",
        "        self.conv2 = PaddedConv2D(out_channels, 3, padding=1)\n",
        "        self.nin_shortcut = (\n",
        "            PaddedConv2D(out_channels, 1)\n",
        "            if in_channels != out_channels\n",
        "            else lambda x: x\n",
        "        )\n",
        "\n",
        "    def call(self, x):\n",
        "        h = self.conv1(keras.activations.swish(self.norm1(x)))\n",
        "        h = self.conv2(keras.activations.swish(self.norm2(h)))\n",
        "        return self.nin_shortcut(x) + h\n"
      ],
      "metadata": {
        "id": "T27yVwEeFMjm"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Encoder"
      ],
      "metadata": {
        "id": "9xSKhMUfFCln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(keras.Sequential):\n",
        "    def __init__(self):\n",
        "        super().__init__(\n",
        "            [\n",
        "                PaddedConv2D(128, 3, padding=1),\n",
        "                ResnetBlock(128, 128),\n",
        "                ResnetBlock(128, 128),\n",
        "                PaddedConv2D(128, 3, padding=(0, 1), stride=2),\n",
        "                ResnetBlock(128, 256),\n",
        "                ResnetBlock(256, 256),\n",
        "                PaddedConv2D(256, 3, padding=(0, 1), stride=2),\n",
        "                ResnetBlock(256, 512),\n",
        "                ResnetBlock(512, 512),\n",
        "                PaddedConv2D(512, 3, padding=(0, 1), stride=2),\n",
        "                ResnetBlock(512, 512),\n",
        "                ResnetBlock(512, 512),\n",
        "                ResnetBlock(512, 512),\n",
        "                AttentionBlock(512),\n",
        "                ResnetBlock(512, 512),\n",
        "                tfa.layers.GroupNormalization(epsilon=1e-5),\n",
        "                keras.layers.Activation(\"swish\"),\n",
        "                PaddedConv2D(8, 3, padding=1),\n",
        "                PaddedConv2D(8, 1),\n",
        "                keras.layers.Lambda(lambda x: x[..., :4] * 0.18215),\n",
        "            ]\n",
        "        )\n"
      ],
      "metadata": {
        "id": "Qq4PseerErGt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Decoder"
      ],
      "metadata": {
        "id": "7eCzMQhnFa0n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(keras.Sequential):\n",
        "    def __init__(self):\n",
        "        super().__init__(\n",
        "            [\n",
        "                keras.layers.Lambda(lambda x: 1 / 0.18215 * x),\n",
        "                PaddedConv2D(4, 1),\n",
        "                PaddedConv2D(512, 3, padding=1),\n",
        "                ResnetBlock(512, 512),\n",
        "                AttentionBlock(512),\n",
        "                ResnetBlock(512, 512),\n",
        "                ResnetBlock(512, 512),\n",
        "                ResnetBlock(512, 512),\n",
        "                ResnetBlock(512, 512),\n",
        "                keras.layers.UpSampling2D(size=(2, 2)),\n",
        "                PaddedConv2D(512, 3, padding=1),\n",
        "                ResnetBlock(512, 512),\n",
        "                ResnetBlock(512, 512),\n",
        "                ResnetBlock(512, 512),\n",
        "                keras.layers.UpSampling2D(size=(2, 2)),\n",
        "                PaddedConv2D(512, 3, padding=1),\n",
        "                ResnetBlock(512, 256),\n",
        "                ResnetBlock(256, 256),\n",
        "                ResnetBlock(256, 256),\n",
        "                keras.layers.UpSampling2D(size=(2, 2)),\n",
        "                PaddedConv2D(256, 3, padding=1),\n",
        "                ResnetBlock(256, 128),\n",
        "                ResnetBlock(128, 128),\n",
        "                ResnetBlock(128, 128),\n",
        "                tfa.layers.GroupNormalization(epsilon=1e-5),\n",
        "                keras.layers.Activation(\"swish\"),\n",
        "                PaddedConv2D(3, 3, padding=1),\n",
        "            ]\n",
        "        )\n",
        "\n"
      ],
      "metadata": {
        "id": "WgnN4qRkFgMd"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CLIP"
      ],
      "metadata": {
        "id": "_RAMKZfhFgv8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clip attention"
      ],
      "metadata": {
        "id": "U27knqnvIXZb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CLIPAttention(keras.layers.Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embed_dim = 768\n",
        "        self.num_heads = 12\n",
        "        self.head_dim = self.embed_dim // self.num_heads\n",
        "        self.scale = self.head_dim**-0.5\n",
        "        self.q_proj = keras.layers.Dense(self.embed_dim)\n",
        "        self.k_proj = keras.layers.Dense(self.embed_dim)\n",
        "        self.v_proj = keras.layers.Dense(self.embed_dim)\n",
        "        self.out_proj = keras.layers.Dense(self.embed_dim)\n",
        "\n",
        "    def _shape(self, tensor, seq_len: int, bsz: int):\n",
        "        a = tf.reshape(tensor, (bsz, seq_len, self.num_heads, self.head_dim))\n",
        "        return keras.layers.Permute((2, 1, 3))(a)  # bs , n_head , seq_len , head_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        hidden_states, causal_attention_mask = inputs\n",
        "        bsz, tgt_len, embed_dim = hidden_states.shape\n",
        "        query_states = self.q_proj(hidden_states) * self.scale\n",
        "        key_states = self._shape(self.k_proj(hidden_states), tgt_len, -1)\n",
        "        value_states = self._shape(self.v_proj(hidden_states), tgt_len, -1)\n",
        "\n",
        "        proj_shape = (-1, tgt_len, self.head_dim)\n",
        "        query_states = self._shape(query_states, tgt_len, -1)\n",
        "        query_states = tf.reshape(query_states, proj_shape)\n",
        "        key_states = tf.reshape(key_states, proj_shape)\n",
        "\n",
        "        src_len = tgt_len\n",
        "        value_states = tf.reshape(value_states, proj_shape)\n",
        "        attn_weights = query_states @ keras.layers.Permute((2, 1))(key_states)\n",
        "\n",
        "        attn_weights = tf.reshape(attn_weights, (-1, self.num_heads, tgt_len, src_len))\n",
        "        attn_weights = attn_weights + causal_attention_mask\n",
        "        attn_weights = tf.reshape(attn_weights, (-1, tgt_len, src_len))\n",
        "\n",
        "        attn_weights = tf.nn.softmax(attn_weights)\n",
        "        attn_output = attn_weights @ value_states\n",
        "\n",
        "        attn_output = tf.reshape(\n",
        "            attn_output, (-1, self.num_heads, tgt_len, self.head_dim)\n",
        "        )\n",
        "        attn_output = keras.layers.Permute((2, 1, 3))(attn_output)\n",
        "        attn_output = tf.reshape(attn_output, (-1, tgt_len, embed_dim))\n",
        "\n",
        "        return self.out_proj(attn_output)\n"
      ],
      "metadata": {
        "id": "VZM6HSapIdlA"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CLIP Encoder Layer"
      ],
      "metadata": {
        "id": "ooBhejHDIhqk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CLIPEncoderLayer(keras.layers.Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layer_norm1 = keras.layers.LayerNormalization(epsilon=1e-5)\n",
        "        self.self_attn = CLIPAttention()\n",
        "        self.layer_norm2 = keras.layers.LayerNormalization(epsilon=1e-5)\n",
        "        self.fc1 = keras.layers.Dense(3072)\n",
        "        self.fc2 = keras.layers.Dense(768)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        hidden_states, causal_attention_mask = inputs\n",
        "        residual = hidden_states\n",
        "\n",
        "        hidden_states = self.layer_norm1(hidden_states)\n",
        "        hidden_states = self.self_attn([hidden_states, causal_attention_mask])\n",
        "        hidden_states = residual + hidden_states\n",
        "\n",
        "        residual = hidden_states\n",
        "        hidden_states = self.layer_norm2(hidden_states)\n",
        "\n",
        "        hidden_states = self.fc1(hidden_states)\n",
        "        hidden_states = quick_gelu(hidden_states)\n",
        "        hidden_states = self.fc2(hidden_states)\n",
        "\n",
        "        return residual + hidden_states\n"
      ],
      "metadata": {
        "id": "XEVy2llPIegZ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##CLIPEncoder"
      ],
      "metadata": {
        "id": "yKsZgVahIm7y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CLIPEncoder(keras.layers.Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layers = [CLIPEncoderLayer() for i in range(12)]\n",
        "\n",
        "    def call(self, inputs):\n",
        "        [hidden_states, causal_attention_mask] = inputs\n",
        "        for l in self.layers:\n",
        "            hidden_states = l([hidden_states, causal_attention_mask])\n",
        "        return hidden_states\n",
        "\n"
      ],
      "metadata": {
        "id": "i8OhCpBrIkMe"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Clip Text embedings"
      ],
      "metadata": {
        "id": "oknw5UFyIxf7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CLIPTextEmbeddings(keras.layers.Layer):\n",
        "    def __init__(self, n_words=77):\n",
        "        super().__init__()\n",
        "        self.token_embedding_layer = keras.layers.Embedding(\n",
        "            49408, 768, name=\"token_embedding\"\n",
        "        )\n",
        "        self.position_embedding_layer = keras.layers.Embedding(\n",
        "            n_words, 768, name=\"position_embedding\"\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        input_ids, position_ids = inputs\n",
        "        word_embeddings = self.token_embedding_layer(input_ids)\n",
        "        position_embeddings = self.position_embedding_layer(position_ids)\n",
        "        return word_embeddings + position_embeddings\n",
        "\n"
      ],
      "metadata": {
        "id": "nTgJPDjqIpwi"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##CLIP Text Transformer"
      ],
      "metadata": {
        "id": "t_uAKZoAI4b2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CLIPTextTransformer(keras.models.Model):\n",
        "    def __init__(self, n_words=77):\n",
        "        super().__init__()\n",
        "        self.embeddings = CLIPTextEmbeddings(n_words=n_words)\n",
        "        self.encoder = CLIPEncoder()\n",
        "        self.final_layer_norm = keras.layers.LayerNormalization(epsilon=1e-5)\n",
        "        self.causal_attention_mask = tf.constant(\n",
        "            np.triu(np.ones((1, 1, 77, 77), dtype=\"float32\") * -np.inf, k=1)\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        input_ids, position_ids = inputs\n",
        "        x = self.embeddings([input_ids, position_ids])\n",
        "        x = self.encoder([x, self.causal_attention_mask])\n",
        "        return self.final_layer_norm(x)\n"
      ],
      "metadata": {
        "id": "K7Qrn7mpI0Z7"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Diiffution Model"
      ],
      "metadata": {
        "id": "DUEYuoE5JU00"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResBlock(keras.layers.Layer):\n",
        "    def __init__(self, channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.in_layers = [\n",
        "            tfa.layers.GroupNormalization(epsilon=1e-5),\n",
        "            keras.activations.swish,\n",
        "            PaddedConv2D(out_channels, 3, padding=1),\n",
        "        ]\n",
        "        self.emb_layers = [\n",
        "            keras.activations.swish,\n",
        "            keras.layers.Dense(out_channels),\n",
        "        ]\n",
        "        self.out_layers = [\n",
        "            tfa.layers.GroupNormalization(epsilon=1e-5),\n",
        "            keras.activations.swish,\n",
        "            PaddedConv2D(out_channels, 3, padding=1),\n",
        "        ]\n",
        "        self.skip_connection = (\n",
        "            PaddedConv2D(out_channels, 1) if channels != out_channels else lambda x: x\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x, emb = inputs\n",
        "        h = apply_seq(x, self.in_layers)\n",
        "        emb_out = apply_seq(emb, self.emb_layers)\n",
        "        h = h + emb_out[:, None, None]\n",
        "        h = apply_seq(h, self.out_layers)\n",
        "        ret = self.skip_connection(x) + h\n",
        "        return ret\n",
        "\n"
      ],
      "metadata": {
        "id": "TdCEfjsRJdMf"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossAttention(keras.layers.Layer):\n",
        "    def __init__(self, n_heads, d_head):\n",
        "        super().__init__()\n",
        "        self.to_q = keras.layers.Dense(n_heads * d_head, use_bias=False)\n",
        "        self.to_k = keras.layers.Dense(n_heads * d_head, use_bias=False)\n",
        "        self.to_v = keras.layers.Dense(n_heads * d_head, use_bias=False)\n",
        "        self.scale = d_head**-0.5\n",
        "        self.num_heads = n_heads\n",
        "        self.head_size = d_head\n",
        "        self.to_out = [keras.layers.Dense(n_heads * d_head)]\n",
        "\n",
        "    def call(self, inputs):\n",
        "        assert type(inputs) is list\n",
        "        if len(inputs) == 1:\n",
        "            inputs = inputs + [None]\n",
        "        x, context = inputs\n",
        "        context = x if context is None else context\n",
        "        q, k, v = self.to_q(x), self.to_k(context), self.to_v(context)\n",
        "        assert len(x.shape) == 3\n",
        "        q = tf.reshape(q, (-1, x.shape[1], self.num_heads, self.head_size))\n",
        "        k = tf.reshape(k, (-1, context.shape[1], self.num_heads, self.head_size))\n",
        "        v = tf.reshape(v, (-1, context.shape[1], self.num_heads, self.head_size))\n",
        "\n",
        "        q = keras.layers.Permute((2, 1, 3))(q)  # (bs, num_heads, time, head_size)\n",
        "        k = keras.layers.Permute((2, 3, 1))(k)  # (bs, num_heads, head_size, time)\n",
        "        v = keras.layers.Permute((2, 1, 3))(v)  # (bs, num_heads, time, head_size)\n",
        "\n",
        "        score = td_dot(q, k) * self.scale\n",
        "        weights = keras.activations.softmax(score)  # (bs, num_heads, time, time)\n",
        "        attention = td_dot(weights, v)\n",
        "        attention = keras.layers.Permute((2, 1, 3))(\n",
        "            attention\n",
        "        )  # (bs, time, num_heads, head_size)\n",
        "        h_ = tf.reshape(attention, (-1, x.shape[1], self.num_heads * self.head_size))\n",
        "        return apply_seq(h_, self.to_out)\n",
        "\n"
      ],
      "metadata": {
        "id": "l_mBHjZvJkGR"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicTransformerBlock(keras.layers.Layer):\n",
        "    def __init__(self, dim, n_heads, d_head):\n",
        "        super().__init__()\n",
        "        self.norm1 = keras.layers.LayerNormalization(epsilon=1e-5)\n",
        "        self.attn1 = CrossAttention(n_heads, d_head)\n",
        "\n",
        "        self.norm2 = keras.layers.LayerNormalization(epsilon=1e-5)\n",
        "        self.attn2 = CrossAttention(n_heads, d_head)\n",
        "\n",
        "        self.norm3 = keras.layers.LayerNormalization(epsilon=1e-5)\n",
        "        self.geglu = GEGLU(dim * 4)\n",
        "        self.dense = keras.layers.Dense(dim)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x, context = inputs\n",
        "        x = self.attn1([self.norm1(x)]) + x\n",
        "        x = self.attn2([self.norm2(x), context]) + x\n",
        "        return self.dense(self.geglu(self.norm3(x))) + x\n"
      ],
      "metadata": {
        "id": "fPQ8GenKJl5w"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SpatialTransformer(keras.layers.Layer):\n",
        "    def __init__(self, channels, n_heads, d_head):\n",
        "        super().__init__()\n",
        "        self.norm = tfa.layers.GroupNormalization(epsilon=1e-5)\n",
        "        assert channels == n_heads * d_head\n",
        "        self.proj_in = PaddedConv2D(n_heads * d_head, 1)\n",
        "        self.transformer_blocks = [BasicTransformerBlock(channels, n_heads, d_head)]\n",
        "        self.proj_out = PaddedConv2D(channels, 1)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x, context = inputs\n",
        "        b, h, w, c = x.shape\n",
        "        x_in = x\n",
        "        x = self.norm(x)\n",
        "        x = self.proj_in(x)\n",
        "        x = tf.reshape(x, (-1, h * w, c))\n",
        "        for block in self.transformer_blocks:\n",
        "            x = block([x, context])\n",
        "        x = tf.reshape(x, (-1, h, w, c))\n",
        "        return self.proj_out(x) + x_in\n",
        "\n"
      ],
      "metadata": {
        "id": "Re1kdIECJoFd"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Downsample(keras.layers.Layer):\n",
        "    def __init__(self, channels):\n",
        "        super().__init__()\n",
        "        self.op = PaddedConv2D(channels, 3, stride=2, padding=1)\n",
        "\n",
        "    def call(self, x):\n",
        "        return self.op(x)\n",
        "\n"
      ],
      "metadata": {
        "id": "auiPPLiBJpsg"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Upsample(keras.layers.Layer):\n",
        "    def __init__(self, channels):\n",
        "        super().__init__()\n",
        "        self.ups = keras.layers.UpSampling2D(size=(2, 2))\n",
        "        self.conv = PaddedConv2D(channels, 3, padding=1)\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.ups(x)\n",
        "        return self.conv(x)\n",
        "\n"
      ],
      "metadata": {
        "id": "rahVc9moJrYB"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class UNetModel(keras.models.Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.time_embed = [\n",
        "            keras.layers.Dense(1280),\n",
        "            keras.activations.swish,\n",
        "            keras.layers.Dense(1280),\n",
        "        ]\n",
        "        self.input_blocks = [\n",
        "            [PaddedConv2D(320, kernel_size=3, padding=1)],\n",
        "            [ResBlock(320, 320), SpatialTransformer(320, 8, 40)],\n",
        "            [ResBlock(320, 320), SpatialTransformer(320, 8, 40)],\n",
        "            [Downsample(320)],\n",
        "            [ResBlock(320, 640), SpatialTransformer(640, 8, 80)],\n",
        "            [ResBlock(640, 640), SpatialTransformer(640, 8, 80)],\n",
        "            [Downsample(640)],\n",
        "            [ResBlock(640, 1280), SpatialTransformer(1280, 8, 160)],\n",
        "            [ResBlock(1280, 1280), SpatialTransformer(1280, 8, 160)],\n",
        "            [Downsample(1280)],\n",
        "            [ResBlock(1280, 1280)],\n",
        "            [ResBlock(1280, 1280)],\n",
        "        ]\n",
        "        self.middle_block = [\n",
        "            ResBlock(1280, 1280),\n",
        "            SpatialTransformer(1280, 8, 160),\n",
        "            ResBlock(1280, 1280),\n",
        "        ]\n",
        "        self.output_blocks = [\n",
        "            [ResBlock(2560, 1280)],\n",
        "            [ResBlock(2560, 1280)],\n",
        "            [ResBlock(2560, 1280), Upsample(1280)],\n",
        "            [ResBlock(2560, 1280), SpatialTransformer(1280, 8, 160)],\n",
        "            [ResBlock(2560, 1280), SpatialTransformer(1280, 8, 160)],\n",
        "            [\n",
        "                ResBlock(1920, 1280),\n",
        "                SpatialTransformer(1280, 8, 160),\n",
        "                Upsample(1280),\n",
        "            ],\n",
        "            [ResBlock(1920, 640), SpatialTransformer(640, 8, 80)],  # 6\n",
        "            [ResBlock(1280, 640), SpatialTransformer(640, 8, 80)],\n",
        "            [\n",
        "                ResBlock(960, 640),\n",
        "                SpatialTransformer(640, 8, 80),\n",
        "                Upsample(640),\n",
        "            ],\n",
        "            [ResBlock(960, 320), SpatialTransformer(320, 8, 40)],\n",
        "            [ResBlock(640, 320), SpatialTransformer(320, 8, 40)],\n",
        "            [ResBlock(640, 320), SpatialTransformer(320, 8, 40)],\n",
        "        ]\n",
        "        self.out = [\n",
        "            tfa.layers.GroupNormalization(epsilon=1e-5),\n",
        "            keras.activations.swish,\n",
        "            PaddedConv2D(4, kernel_size=3, padding=1),\n",
        "        ]\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x, t_emb, context = inputs\n",
        "        emb = apply_seq(t_emb, self.time_embed)\n",
        "\n",
        "        def apply(x, layer):\n",
        "            if isinstance(layer, ResBlock):\n",
        "                x = layer([x, emb])\n",
        "            elif isinstance(layer, SpatialTransformer):\n",
        "                x = layer([x, context])\n",
        "            else:\n",
        "                x = layer(x)\n",
        "            return x\n",
        "\n",
        "        saved_inputs = []\n",
        "        for b in self.input_blocks:\n",
        "            for layer in b:\n",
        "                x = apply(x, layer)\n",
        "            saved_inputs.append(x)\n",
        "\n",
        "        for layer in self.middle_block:\n",
        "            x = apply(x, layer)\n",
        "\n",
        "        for b in self.output_blocks:\n",
        "            x = tf.concat([x, saved_inputs.pop()], axis=-1)\n",
        "            for layer in b:\n",
        "                x = apply(x, layer)\n",
        "        return apply_seq(x, self.out)\n"
      ],
      "metadata": {
        "id": "aUUD4ZOiJstj"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CLIP tokenizor"
      ],
      "metadata": {
        "id": "AZG3f-PfLPWW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "\n",
        "#Unzip the downloaded file\n",
        "zip_ref = zipfile.ZipFile(\"/content/drive/MyDrive/clip_tokenizer.zip\", \"r\")\n",
        "zip_ref.extractall()\n",
        "zip_ref.close()\n"
      ],
      "metadata": {
        "id": "Ku95esKOPNAC"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# stable diffusion"
      ],
      "metadata": {
        "id": "ZlBAbvR0JuZg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from clip_tokenizer import SimpleTokenizer"
      ],
      "metadata": {
        "id": "F6fOr0ZPNMrl"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_TEXT_LEN = 77\n",
        "\n",
        "class StableDiffusion:\n",
        "    def __init__(\n",
        "        self, img_height=1000, img_width=1000, jit_compile=False, download_weights=True\n",
        "    ):\n",
        "        self.img_height = img_height\n",
        "        self.img_width = img_width\n",
        "        self.tokenizer = SimpleTokenizer()\n",
        "\n",
        "        text_encoder, diffusion_model, decoder, encoder = get_models(\n",
        "            img_height, img_width, download_weights=download_weights\n",
        "        )\n",
        "        self.text_encoder = text_encoder\n",
        "        self.diffusion_model = diffusion_model\n",
        "        self.decoder = decoder\n",
        "        self.encoder = encoder\n",
        "\n",
        "        if jit_compile:\n",
        "            self.text_encoder.compile(jit_compile=True)\n",
        "            self.diffusion_model.compile(jit_compile=True)\n",
        "            self.decoder.compile(jit_compile=True)\n",
        "            self.encoder.compile(jit_compile=True)\n",
        "\n",
        "        self.dtype = tf.float32\n",
        "        if tf.keras.mixed_precision.global_policy().name == \"mixed_float16\":\n",
        "            self.dtype = tf.float16\n",
        "\n",
        "    def generate(\n",
        "        self,\n",
        "        prompt,\n",
        "        negative_prompt=None,\n",
        "        batch_size=1,\n",
        "        num_steps=25,\n",
        "        unconditional_guidance_scale=7.5,\n",
        "        temperature=1,\n",
        "        seed=None,\n",
        "        input_image=None,\n",
        "        input_mask=None,\n",
        "        input_image_strength=0.5,\n",
        "    ):\n",
        "        # Tokenize prompt (i.e. starting context)\n",
        "        inputs = self.tokenizer.encode(prompt)\n",
        "        assert len(inputs) < 77, \"Prompt is too long (should be < 77 tokens)\"\n",
        "        phrase = inputs + [49407] * (77 - len(inputs))\n",
        "        phrase = np.array(phrase)[None].astype(\"int32\")\n",
        "        phrase = np.repeat(phrase, batch_size, axis=0)\n",
        "\n",
        "        # Encode prompt tokens (and their positions) into a \"context vector\"\n",
        "        pos_ids = np.array(list(range(77)))[None].astype(\"int32\")\n",
        "        pos_ids = np.repeat(pos_ids, batch_size, axis=0)\n",
        "        context = self.text_encoder.predict_on_batch([phrase, pos_ids])\n",
        "\n",
        "        input_image_tensor = None\n",
        "        if input_image is not None:\n",
        "            if type(input_image) is str:\n",
        "                input_image = Image.open(input_image)\n",
        "                input_image = input_image.resize((self.img_width, self.img_height))\n",
        "\n",
        "            elif type(input_image) is np.ndarray:\n",
        "                input_image = np.resize(\n",
        "                    input_image, (self.img_height, self.img_width, input_image.shape[2])\n",
        "                )\n",
        "\n",
        "            input_image_array = np.array(input_image, dtype=np.float32)[None, ..., :3]\n",
        "            input_image_tensor = tf.cast(\n",
        "                (input_image_array / 255.0) * 2 - 1, self.dtype\n",
        "            )\n",
        "\n",
        "        if type(input_mask) is str:\n",
        "            input_mask = Image.open(input_mask)\n",
        "            input_mask = input_mask.resize((self.img_width, self.img_height))\n",
        "            input_mask_array = np.array(input_mask, dtype=np.float32)[None, ..., None]\n",
        "            input_mask_array = input_mask_array / 255.0\n",
        "\n",
        "            latent_mask = input_mask.resize((self.img_width // 8, self.img_height // 8))\n",
        "            latent_mask = np.array(latent_mask, dtype=np.float32)[None, ..., None]\n",
        "            latent_mask = 1 - (latent_mask.astype(\"float\") / 255.0)\n",
        "            latent_mask_tensor = tf.cast(\n",
        "                tf.repeat(latent_mask, batch_size, axis=0), self.dtype\n",
        "            )\n",
        "\n",
        "        # Tokenize negative prompt or use default padding tokens\n",
        "        unconditional_tokens = _UNCONDITIONAL_TOKENS\n",
        "        if negative_prompt is not None:\n",
        "            inputs = self.tokenizer.encode(negative_prompt)\n",
        "            assert (\n",
        "                len(inputs) < 77\n",
        "            ), \"Negative prompt is too long (should be < 77 tokens)\"\n",
        "            unconditional_tokens = inputs + [49407] * (77 - len(inputs))\n",
        "\n",
        "        # Encode unconditional tokens (and their positions into an\n",
        "        # \"unconditional context vector\"\n",
        "        unconditional_tokens = np.array(unconditional_tokens)[None].astype(\"int32\")\n",
        "        unconditional_tokens = np.repeat(unconditional_tokens, batch_size, axis=0)\n",
        "        unconditional_context = self.text_encoder.predict_on_batch(\n",
        "            [unconditional_tokens, pos_ids]\n",
        "        )\n",
        "        timesteps = np.arange(1, 1000, 1000 // num_steps)\n",
        "        input_img_noise_t = timesteps[int(len(timesteps) * input_image_strength)]\n",
        "        latent, alphas, alphas_prev = self.get_starting_parameters(\n",
        "            timesteps,\n",
        "            batch_size,\n",
        "            seed,\n",
        "            input_image=input_image_tensor,\n",
        "            input_img_noise_t=input_img_noise_t,\n",
        "        )\n",
        "\n",
        "        if input_image is not None:\n",
        "            timesteps = timesteps[: int(len(timesteps) * input_image_strength)]\n",
        "\n",
        "        # Diffusion stage\n",
        "        progbar = tqdm(list(enumerate(timesteps))[::-1])\n",
        "        for index, timestep in progbar:\n",
        "            progbar.set_description(f\"{index:3d} {timestep:3d}\")\n",
        "            e_t = self.get_model_output(\n",
        "                latent,\n",
        "                timestep,\n",
        "                context,\n",
        "                unconditional_context,\n",
        "                unconditional_guidance_scale,\n",
        "                batch_size,\n",
        "            )\n",
        "            a_t, a_prev = alphas[index], alphas_prev[index]\n",
        "            latent, pred_x0 = self.get_x_prev_and_pred_x0(\n",
        "                latent, e_t, index, a_t, a_prev, temperature, seed\n",
        "            )\n",
        "\n",
        "            if input_mask is not None and input_image is not None:\n",
        "                # If mask is provided, noise at current timestep will be added to input image.\n",
        "                # The intermediate latent will be merged with input latent.\n",
        "                latent_orgin, alphas, alphas_prev = self.get_starting_parameters(\n",
        "                    timesteps,\n",
        "                    batch_size,\n",
        "                    seed,\n",
        "                    input_image=input_image_tensor,\n",
        "                    input_img_noise_t=timestep,\n",
        "                )\n",
        "                latent = latent_orgin * latent_mask_tensor + latent * (\n",
        "                    1 - latent_mask_tensor\n",
        "                )\n",
        "\n",
        "        # Decoding stage\n",
        "        decoded = self.decoder.predict_on_batch(latent)\n",
        "        decoded = ((decoded + 1) / 2) * 255\n",
        "\n",
        "        if input_mask is not None:\n",
        "            # Merge inpainting output with original image\n",
        "            decoded = (\n",
        "                input_image_array * (1 - input_mask_array)\n",
        "                + np.array(decoded) * input_mask_array\n",
        "            )\n",
        "\n",
        "        return np.clip(decoded, 0, 255).astype(\"uint8\")\n",
        "\n",
        "    def timestep_embedding(self, timesteps, dim=320, max_period=10000):\n",
        "        half = dim // 2\n",
        "        freqs = np.exp(\n",
        "            -math.log(max_period) * np.arange(0, half, dtype=\"float32\") / half\n",
        "        )\n",
        "        args = np.array(timesteps) * freqs\n",
        "        embedding = np.concatenate([np.cos(args), np.sin(args)])\n",
        "        return tf.convert_to_tensor(embedding.reshape(1, -1), dtype=self.dtype)\n",
        "\n",
        "    def add_noise(self, x, t, noise=None):\n",
        "        batch_size, w, h = x.shape[0], x.shape[1], x.shape[2]\n",
        "        if noise is None:\n",
        "            noise = tf.random.normal((batch_size, w, h, 4), dtype=self.dtype)\n",
        "        sqrt_alpha_prod = _ALPHAS_CUMPROD[t] ** 0.5\n",
        "        sqrt_one_minus_alpha_prod = (1 - _ALPHAS_CUMPROD[t]) ** 0.5\n",
        "\n",
        "        return sqrt_alpha_prod * x + sqrt_one_minus_alpha_prod * noise\n",
        "\n",
        "    def get_starting_parameters(\n",
        "        self, timesteps, batch_size, seed, input_image=None, input_img_noise_t=None\n",
        "    ):\n",
        "        n_h = self.img_height // 8\n",
        "        n_w = self.img_width // 8\n",
        "        alphas = [_ALPHAS_CUMPROD[t] for t in timesteps]\n",
        "        alphas_prev = [1.0] + alphas[:-1]\n",
        "        if input_image is None:\n",
        "            latent = tf.random.normal((batch_size, n_h, n_w, 4), seed=seed)\n",
        "        else:\n",
        "            latent = self.encoder(input_image)\n",
        "            latent = tf.repeat(latent, batch_size, axis=0)\n",
        "            latent = self.add_noise(latent, input_img_noise_t)\n",
        "        return latent, alphas, alphas_prev\n",
        "\n",
        "    def get_model_output(\n",
        "        self,\n",
        "        latent,\n",
        "        t,\n",
        "        context,\n",
        "        unconditional_context,\n",
        "        unconditional_guidance_scale,\n",
        "        batch_size,\n",
        "    ):\n",
        "        timesteps = np.array([t])\n",
        "        t_emb = self.timestep_embedding(timesteps)\n",
        "        t_emb = np.repeat(t_emb, batch_size, axis=0)\n",
        "        unconditional_latent = self.diffusion_model.predict_on_batch(\n",
        "            [latent, t_emb, unconditional_context]\n",
        "        )\n",
        "        latent = self.diffusion_model.predict_on_batch([latent, t_emb, context])\n",
        "        return unconditional_latent + unconditional_guidance_scale * (\n",
        "            latent - unconditional_latent\n",
        "        )\n",
        "\n",
        "    def get_x_prev_and_pred_x0(self, x, e_t, index, a_t, a_prev, temperature, seed):\n",
        "        sigma_t = 0\n",
        "        sqrt_one_minus_at = math.sqrt(1 - a_t)\n",
        "        pred_x0 = (x - sqrt_one_minus_at * e_t) / math.sqrt(a_t)\n",
        "\n",
        "        # Direction pointing to x_t\n",
        "        dir_xt = math.sqrt(1.0 - a_prev - sigma_t**2) * e_t\n",
        "        noise = sigma_t * tf.random.normal(x.shape, seed=seed) * temperature\n",
        "        x_prev = math.sqrt(a_prev) * pred_x0 + dir_xt\n",
        "        return x_prev, pred_x0\n",
        "\n",
        "    def load_weights_from_pytorch_ckpt(self, pytorch_ckpt_path):\n",
        "        import torch\n",
        "\n",
        "        pt_weights = torch.load(pytorch_ckpt_path, map_location=\"cpu\")\n",
        "        for module_name in [\"text_encoder\", \"diffusion_model\", \"decoder\", \"encoder\"]:\n",
        "            module_weights = []\n",
        "            for i, (key, perm) in enumerate(PYTORCH_CKPT_MAPPING[module_name]):\n",
        "                w = pt_weights[\"state_dict\"][key].numpy()\n",
        "                if perm is not None:\n",
        "                    w = np.transpose(w, perm)\n",
        "                module_weights.append(w)\n",
        "            getattr(self, module_name).set_weights(module_weights)\n",
        "            print(\"Loaded %d weights for %s\" % (len(module_weights), module_name))\n",
        "\n"
      ],
      "metadata": {
        "id": "UN-2dM0TKLRh"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_models(img_height, img_width, download_weights=True):\n",
        "    n_h = img_height // 8\n",
        "    n_w = img_width // 8\n",
        "\n",
        "    # Create text encoder\n",
        "    input_word_ids = keras.layers.Input(shape=(MAX_TEXT_LEN,), dtype=\"int32\")\n",
        "    input_pos_ids = keras.layers.Input(shape=(MAX_TEXT_LEN,), dtype=\"int32\")\n",
        "    embeds = CLIPTextTransformer()([input_word_ids, input_pos_ids])\n",
        "    text_encoder = keras.models.Model([input_word_ids, input_pos_ids], embeds)\n",
        "\n",
        "    # Creation diffusion UNet\n",
        "    context = keras.layers.Input((MAX_TEXT_LEN, 768))\n",
        "    t_emb = keras.layers.Input((320,))\n",
        "    latent = keras.layers.Input((n_h, n_w, 4))\n",
        "    unet = UNetModel()\n",
        "    diffusion_model = keras.models.Model(\n",
        "        [latent, t_emb, context], unet([latent, t_emb, context])\n",
        "    )\n",
        "\n",
        "    # Create decoder\n",
        "    latent = keras.layers.Input((n_h, n_w, 4))\n",
        "    decoder = Decoder()\n",
        "    decoder = keras.models.Model(latent, decoder(latent))\n",
        "\n",
        "    inp_img = keras.layers.Input((img_height, img_width, 3))\n",
        "    encoder = Encoder()\n",
        "    encoder = keras.models.Model(inp_img, encoder(inp_img))\n",
        "\n",
        "    if download_weights:\n",
        "        text_encoder_weights_fpath = keras.utils.get_file(\n",
        "            origin=\"https://huggingface.co/fchollet/stable-diffusion/resolve/main/text_encoder.h5\",\n",
        "            file_hash=\"d7805118aeb156fc1d39e38a9a082b05501e2af8c8fbdc1753c9cb85212d6619\",\n",
        "        )\n",
        "        diffusion_model_weights_fpath = keras.utils.get_file(\n",
        "            origin=\"https://huggingface.co/fchollet/stable-diffusion/resolve/main/diffusion_model.h5\",\n",
        "            file_hash=\"a5b2eea58365b18b40caee689a2e5d00f4c31dbcb4e1d58a9cf1071f55bbbd3a\",\n",
        "        )\n",
        "        decoder_weights_fpath = keras.utils.get_file(\n",
        "            origin=\"https://huggingface.co/fchollet/stable-diffusion/resolve/main/decoder.h5\",\n",
        "            file_hash=\"6d3c5ba91d5cc2b134da881aaa157b2d2adc648e5625560e3ed199561d0e39d5\",\n",
        "        )\n",
        "\n",
        "        encoder_weights_fpath = keras.utils.get_file(\n",
        "            origin=\"https://huggingface.co/divamgupta/stable-diffusion-tensorflow/resolve/main/encoder_newW.h5\",\n",
        "            file_hash=\"56a2578423c640746c5e90c0a789b9b11481f47497f817e65b44a1a5538af754\",\n",
        "        )\n",
        "\n",
        "        text_encoder.load_weights(text_encoder_weights_fpath)\n",
        "        diffusion_model.load_weights(diffusion_model_weights_fpath)\n",
        "        decoder.load_weights(decoder_weights_fpath)\n",
        "        encoder.load_weights(encoder_weights_fpath)\n",
        "    return text_encoder, diffusion_model, decoder, encoder\n"
      ],
      "metadata": {
        "id": "yrmQzjXKKcUa"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Prompting"
      ],
      "metadata": {
        "id": "F02elo1zaHmV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generator = StableDiffusion(img_height=512, img_width=512, jit_compile=False)\n",
        "img = generator.generate(\n",
        "    \"a painting of a virus monster playing guitar\",\n",
        "    negative_prompt=\"\",\n",
        "    num_steps=50,\n",
        "    unconditional_guidance_scale=7.5,\n",
        "    temperature=1,\n",
        "    batch_size=1,\n",
        "    # seed=args.seed,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VSEa7ZWCPZR2",
        "outputId": "58508781-c274-4dd2-c3a5-07d9bcbc05a5"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 49 981:   0%|          | 0/50 [00:21<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ResourceExhaustedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-aa697d31a1ff>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStableDiffusion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_height\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_width\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjit_compile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m img = generator.generate(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;34m\"a painting of a virus monster playing guitar\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mnegative_prompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mnum_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-d2ea16d70ba8>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, prompt, negative_prompt, batch_size, num_steps, unconditional_guidance_scale, temperature, seed, input_image, input_mask, input_image_strength)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimestep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprogbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_description\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{index:3d} {timestep:3d}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             e_t = self.get_model_output(\n\u001b[0m\u001b[1;32m    117\u001b[0m                 \u001b[0mlatent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                 \u001b[0mtimestep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-d2ea16d70ba8>\u001b[0m in \u001b[0;36mget_model_output\u001b[0;34m(self, latent, t, context, unconditional_context, unconditional_guidance_scale, batch_size)\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mt_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimestep_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimesteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0mt_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         unconditional_latent = self.diffusion_model.predict_on_batch(\n\u001b[0m\u001b[1;32m    202\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0mlatent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munconditional_context\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mpredict_on_batch\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   2878\u001b[0m             )\n\u001b[1;32m   2879\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_predict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2880\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2881\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2882\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node model_9/u_net_model_2/spatial_transformer_32/basic_transformer_block_32/cross_attention_64/Softmax defined at (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n\n  File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 195, in start\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 685, in <lambda>\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 738, in _run_callback\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 825, in inner\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"<ipython-input-37-aa697d31a1ff>\", line 2, in <cell line: 2>\n\n  File \"<ipython-input-23-d2ea16d70ba8>\", line 116, in generate\n\n  File \"<ipython-input-23-d2ea16d70ba8>\", line 201, in get_model_output\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2880, in predict_on_batch\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2440, in predict_function\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2425, in step_function\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2413, in run_step\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2381, in predict_step\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 590, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/functional.py\", line 515, in call\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/functional.py\", line 672, in _run_internal_graph\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 590, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"<ipython-input-20-5a02dd6e73c0>\", line 70, in call\n\n  File \"<ipython-input-20-5a02dd6e73c0>\", line 71, in call\n\n  File \"<ipython-input-20-5a02dd6e73c0>\", line 72, in call\n\n  File \"<ipython-input-20-5a02dd6e73c0>\", line 61, in apply\n\n  File \"<ipython-input-20-5a02dd6e73c0>\", line 63, in apply\n\n  File \"<ipython-input-20-5a02dd6e73c0>\", line 64, in apply\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"<ipython-input-17-e59589cba952>\", line 17, in call\n\n  File \"<ipython-input-17-e59589cba952>\", line 18, in call\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"<ipython-input-16-2c898181715d>\", line 16, in call\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"<ipython-input-15-c6e8bd2d325d>\", line 29, in call\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/activations.py\", line 87, in softmax\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend.py\", line 5448, in softmax\n\nOOM when allocating tensor with shape[1,8,4096,4096] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node model_9/u_net_model_2/spatial_transformer_32/basic_transformer_block_32/cross_attention_64/Softmax}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_predict_function_125899]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pnginfo = PngInfo()\n",
        "output_image_name = \"x.png\"\n",
        "Image.fromarray(img[0]).save(output_image_name, pnginfo=pnginfo)\n",
        "print(f\"saved at {output_image_name}\")\n"
      ],
      "metadata": {
        "id": "eSH-eWVKYCEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "utn_iOf7abuf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}